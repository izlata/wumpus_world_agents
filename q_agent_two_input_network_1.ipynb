{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wumpus World - DeepQAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action-value network with two inputs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-learning: using an action-value network with two inputs (the states and actions) and one output (the action-value). Epsilon-greedy policy was used.\n",
    "\n",
    "The encoded state goes through several convolutional layers. The proposed action goes into a separate input. The output of the convolutional layers is combined with the proposed action and passed through a dense layer.\n",
    "\n",
    "Belief state is encoded as a 3-D tensor using 13 feature planes (each plane is a grid_height x grid_width matrix). The state shape is (13, grid_height, grid_width). See the `DeepQAgent.encode_belief_state` function.\n",
    "- Plane 1 - location of the agent\n",
    "- Plane 2 - visited locations\n",
    "- Plane 3 - stench locations\n",
    "- Plane 4 - breeze locations\n",
    "- Planes 5-8 - orientation of the agent\n",
    "- Plane 9 - does the agent have gold?\n",
    "- Plane 10 - does the agent perceive a glitter?\n",
    "- Plane 11 - does the agent have an arrow?\n",
    "- Plane 12 - have the agent heard a scream?\n",
    "- Plane 13 - does the agent perceives a bump?\n",
    "\n",
    "Th experience data generated by the probabilistic agent ProbAgent was used as the first experience set to train the DeepQAgent. The DeepQAgent learned to climb out of the cave without the gold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The `Orientation` class: orientation of the Agent (north, south, east, west)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "tkhP0hufZ2Ct"
   },
   "outputs": [],
   "source": [
    "import enum\n",
    "\n",
    "\n",
    "class Orientation(enum.Enum):\n",
    "    north = 1\n",
    "    south = 2\n",
    "    east = 3\n",
    "    west = 4\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def turn_left(self):\n",
    "        dict_turn_left = {\n",
    "            Orientation.north: Orientation.west, \n",
    "            Orientation.south: Orientation.east, \n",
    "            Orientation.east: Orientation.north, \n",
    "            Orientation.west: Orientation.south\n",
    "        }\n",
    "        new_orientation = dict_turn_left.get(self)\n",
    "        return new_orientation\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def turn_right(self):\n",
    "        dict_turn_right = {\n",
    "            Orientation.north: Orientation.east, \n",
    "            Orientation.south: Orientation.west, \n",
    "            Orientation.east: Orientation.south, \n",
    "            Orientation.west: Orientation.north\n",
    "        }\n",
    "        new_orientation = dict_turn_right.get(self)\n",
    "        return new_orientation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The `Action` class**\n",
    "\n",
    "The agent can move forward, turn left by 90 degrees, or turn right by 90 degrees. \n",
    "\n",
    "The action Grab can be used to pick up the gold if it is in the same square as the agent. \n",
    "\n",
    "The action Shoot can be used to fire an arrow in a straight line in the direction the agent is facing. the arrow continues until it either kills the Wumpus or hits a wall. The Agent has only one arrow. \n",
    "\n",
    "The action Climb, can be used to climb out of the cave, but only from the start square."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "IKX-bA7sZ2Ct"
   },
   "outputs": [],
   "source": [
    "class Action():\n",
    "    def __init__(self, is_forward=False, is_turn_left=False, is_turn_right=False, \n",
    "                 is_shoot=False, is_grab=False, is_climb=False):\n",
    "        assert is_forward ^ is_turn_left ^ is_turn_right ^ is_shoot ^ is_grab ^ is_climb\n",
    "        self.is_forward = is_forward\n",
    "        self.is_turn_left = is_turn_left\n",
    "        self.is_turn_right = is_turn_right\n",
    "        self.is_shoot = is_shoot\n",
    "        self.is_grab = is_grab\n",
    "        self.is_climb = is_climb\n",
    "    \n",
    "    @classmethod\n",
    "    def forward(cls):\n",
    "        return Action(is_forward=True)\n",
    "    \n",
    "    @classmethod\n",
    "    def turn_left(cls):\n",
    "        return Action(is_turn_left=True)\n",
    "    \n",
    "    @classmethod\n",
    "    def turn_right(cls):\n",
    "        return Action(is_turn_right=True)\n",
    "    \n",
    "    @classmethod\n",
    "    def shoot(cls):\n",
    "        return Action(is_shoot=True)\n",
    "    \n",
    "    @classmethod\n",
    "    def grab(cls):\n",
    "        return Action(is_grab=True)\n",
    "    \n",
    "    @classmethod\n",
    "    def climb(cls):\n",
    "        return Action(is_climb=True)\n",
    "    \n",
    "    def show(self):\n",
    "        if self.is_forward:\n",
    "            action_str = \"forward\"\n",
    "        elif self.is_turn_left:\n",
    "            action_str = \"turn_left\"\n",
    "        elif self.is_turn_right:\n",
    "            action_str = \"turn_right\"\n",
    "        elif self.is_shoot:\n",
    "            action_str = \"shoot\"\n",
    "        elif self.is_grab:\n",
    "            action_str = \"grab\"\n",
    "        else:\n",
    "            action_str = \"climb\"\n",
    "        return action_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The `Coords` class** \n",
    "\n",
    "Each square on the grid has two coordinates: x (column) and y (row). The start square is `Coords(x=0, y=0)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "M4f_M3FEZ2Cu"
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "class Coords(namedtuple('Coords', 'x y')):\n",
    "    def adjacent_cells(self, grid_width, grid_height):\n",
    "        neighbors = []\n",
    "        if self.x > 0: # to left\n",
    "            neighbors.append(Coords(self.x - 1, self.y))\n",
    "        if self.x < (grid_width - 1): # to right\n",
    "            neighbors.append(Coords(self.x + 1, self.y))\n",
    "        if self.y > 0: # below\n",
    "            neighbors.append(Coords(self.x, self.y - 1))\n",
    "        if self.y < (grid_height - 1): # above\n",
    "            neighbors.append(Coords(self.x, self.y + 1))\n",
    "        return neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The `Percept` class**\n",
    "\n",
    "The Agent has five sensors.\n",
    "\n",
    "- In the square containing the Wumpus and in the directly (not diagonally) adjacent squares, the Agent will receive a Stench.\n",
    "- In the squares directly adjacent to a pit, the Agent will perceive a Breeze.\n",
    "- In the square where the gold is, the Agent will perceive a Glitter.\n",
    "- When an Agent walks into a wall it will perceive a Bump.\n",
    "- When the Wumpus is killed, the Agent will hear a Scream.\n",
    "\n",
    "The percept also contains the reward calculated by the environment after each agent's action : +1000 for climbing out of the cave with the gold, -1000 for falling into a pit or being eaten by the Wumpus, -1 for each action taken and -10 for using the arrow.\n",
    "\n",
    "`Percept.is_terminated`: The game ends either when the Agent dies or when the Agent climbs out of the cave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "O7Q1t6vVZ2Cu"
   },
   "outputs": [],
   "source": [
    "class Percept():\n",
    "    def __init__(self, stench, breeze, glitter, bump, scream, is_terminated, reward):\n",
    "        self.stench = stench\n",
    "        self.breeze = breeze\n",
    "        self.glitter = glitter\n",
    "        self.bump = bump\n",
    "        self.scream = scream\n",
    "        self.is_terminated = is_terminated\n",
    "        self.reward = reward\n",
    "    \n",
    "    def show(self):\n",
    "        print(\"stench: {}, breeze: {}, glitter: {}, bump: {}, scream: {}, is_terminated: {}, reward: {}\"\n",
    "              .format(self.stench, self.breeze, self.glitter, self.bump, self.scream, self.is_terminated, self.reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The `AgentState` class**\n",
    "\n",
    "Information about the Agent: location, orientation and whether the Agent is alive, has gold and has arrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "4h9b2yVKZ2Cu"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "class AgentState():\n",
    "    def __init__(self, location=Coords(0, 0), orientation=Orientation.east, has_gold=False, has_arrow=True, is_alive=True):\n",
    "        self.location = location\n",
    "        self.orientation = orientation\n",
    "        self.has_gold = has_gold\n",
    "        self.has_arrow = has_arrow\n",
    "        self.is_alive = is_alive\n",
    "    \n",
    "    def turn_left(self):\n",
    "        new_state = copy.deepcopy(self)\n",
    "        new_state.orientation = new_state.orientation.turn_left\n",
    "        return new_state\n",
    "    \n",
    "    def turn_right(self):\n",
    "        new_state = copy.deepcopy(self)\n",
    "        new_state.orientation = new_state.orientation.turn_right\n",
    "        return new_state\n",
    "    \n",
    "    def forward(self, grid_width, grid_height):\n",
    "        if self.orientation == Orientation.north:\n",
    "            new_loc = Coords(self.location.x, min(grid_height - 1, self.location.y + 1))\n",
    "        elif self.orientation == Orientation.south:\n",
    "            new_loc = Coords(self.location.x, max(0, self.location.y - 1))\n",
    "        elif self.orientation == Orientation.east:\n",
    "            new_loc = Coords(min(grid_width - 1, self.location.x + 1), self.location.y)\n",
    "        else:\n",
    "            new_loc = Coords(max(0, self.location.x - 1), self.location.y) # if Orientation.west\n",
    "        new_state = copy.deepcopy(self)\n",
    "        new_state.location = new_loc\n",
    "        return new_state\n",
    "    \n",
    "    def apply_move_action(self, action, grid_width, grid_height):\n",
    "        if action.is_forward:\n",
    "            return self.forward(grid_width, grid_height)\n",
    "        if action.is_turn_left:\n",
    "            return self.turn_left()\n",
    "        if action.is_turn_right:\n",
    "            return self.turn_right()\n",
    "        if action.is_shoot:\n",
    "            return self.use_arrow()\n",
    "        if action.is_climb:\n",
    "            return self\n",
    "    \n",
    "    def use_arrow(self):\n",
    "        new_state = copy.deepcopy(self)\n",
    "        new_state.has_arrow = False\n",
    "        return new_state\n",
    "    \n",
    "    def show(self):\n",
    "        print(\"location: {}, orientation: {}, has_gold: {}, has_arrow: {}, is_alive: {}\"\n",
    "              .format(self.location, self.orientation, self.has_gold, self.has_arrow, self.is_alive))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functions to create the list of all locations on the board and to generate the locations of gold, wumpus and pits**\n",
    "\n",
    "The locations of the gold and the Wumpus are chosen randomly, with a uniform distribution, from the squares other than the start square. \n",
    "\n",
    "Each square other than the start can be a pit, with probability = `pit_prob`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "0wfqYWG6Z2Cu"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "# Create a list with all locations\n",
    "\n",
    "def list_all_locations(grid_width, grid_height):\n",
    "    all_cells = []\n",
    "    for x in range(grid_width):\n",
    "        for y in range(grid_height):\n",
    "            all_cells.append(Coords(x, y))\n",
    "    return all_cells\n",
    "\n",
    "\n",
    "\n",
    "# Create locations for gold and wumpus\n",
    "\n",
    "def random_location_except_origin(grid_width, grid_height):\n",
    "    locations = list_all_locations(grid_width, grid_height)\n",
    "    locations.remove(Coords(0, 0))\n",
    "    return random.choice(locations)\n",
    "\n",
    "\n",
    "\n",
    "# Create pit locations\n",
    "\n",
    "def create_pit_locations(grid_width, grid_height, pit_prob):\n",
    "    locations = list_all_locations(grid_width, grid_height)\n",
    "    locations.remove(Coords(0, 0))\n",
    "    pit_locations = [loc for loc in locations if random.random() < pit_prob]\n",
    "    return pit_locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The `Environment` class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "CP7K90GpZ2Cu"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "class Environment():\n",
    "    def __init__(self, grid_width, grid_height, pit_prob, allow_climb_without_gold, agent, pit_locations,\n",
    "                 terminated, wumpus_loc, wumpus_alive, gold_loc):\n",
    "        self.grid_width = grid_width\n",
    "        self.grid_height = grid_height\n",
    "        self.pit_prob = pit_prob\n",
    "        self.allow_climb_without_gold = allow_climb_without_gold\n",
    "        self.agent = agent\n",
    "        self.pit_locations = pit_locations\n",
    "        self.terminated = terminated\n",
    "        self.wumpus_loc = wumpus_loc\n",
    "        self.wumpus_alive = wumpus_alive\n",
    "        self.gold_loc = gold_loc\n",
    "    \n",
    "    \n",
    "    def is_pit_at(self, coords):\n",
    "        return coords in self.pit_locations\n",
    "    \n",
    "    \n",
    "    def is_wumpus_at(self, coords):\n",
    "        return coords == self.wumpus_loc\n",
    "    \n",
    "    \n",
    "    def is_agent_at(self, coords):\n",
    "        return coords == self.agent.location\n",
    "    \n",
    "    \n",
    "    def is_glitter(self):\n",
    "        return self.gold_loc == self.agent.location\n",
    "    \n",
    "    \n",
    "    def is_gold_at(self, coords):\n",
    "        return coords == self.gold_loc\n",
    "    \n",
    "    \n",
    "    def wumpus_in_line_of_fire(self):\n",
    "        if self.agent.orientation == Orientation.west:\n",
    "            return self.agent.location.x > self.wumpus_loc.x and self.agent.location.y == self.wumpus_loc.y\n",
    "        if self.agent.orientation == Orientation.east:\n",
    "            return self.agent.location.x < self.wumpus_loc.x and self.agent.location.y == self.wumpus_loc.y\n",
    "        if self.agent.orientation == Orientation.south:\n",
    "            return self.agent.location.x == self.wumpus_loc.x and self.agent.location.y > self.wumpus_loc.y\n",
    "        if self.agent.orientation == Orientation.north:\n",
    "            return self.agent.location.x == self.wumpus_loc.x and self.agent.location.y < self.wumpus_loc.y\n",
    "    \n",
    "    \n",
    "    def kill_attempt_successful(self):\n",
    "        return self.agent.has_arrow and self.wumpus_alive and self.wumpus_in_line_of_fire()\n",
    "    \n",
    "    \n",
    "    def is_pit_adjacent(self, coords):\n",
    "        for cell in coords.adjacent_cells(self.grid_width, self.grid_height):\n",
    "            if cell in self.pit_locations:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    \n",
    "    def is_wumpus_adjacent(self, coords):\n",
    "        for cell in coords.adjacent_cells(self.grid_width, self.grid_height):\n",
    "            if self.is_wumpus_at(cell):\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    \n",
    "    def is_breeze(self):\n",
    "        return self.is_pit_adjacent(self.agent.location)\n",
    "    \n",
    "    \n",
    "    def is_stench(self):\n",
    "        return self.is_wumpus_adjacent(self.agent.location) or self.is_wumpus_at(self.agent.location)\n",
    "    \n",
    "    \n",
    "    def apply_action(self, action):\n",
    "        if self.terminated:\n",
    "            return (self, Percept(False, False, False, False, False, True, 0))\n",
    "        else:\n",
    "            if action.is_forward:\n",
    "                moved_agent = self.agent.forward(self.grid_width, self.grid_height)\n",
    "                death = (self.is_wumpus_at(moved_agent.location) and self.wumpus_alive) or self.is_pit_at(moved_agent.location)\n",
    "                new_agent = copy.deepcopy(moved_agent)\n",
    "                new_agent.is_alive = not death\n",
    "                new_gold_loc = new_agent.location if self.agent.has_gold else self.gold_loc\n",
    "                new_env = Environment(self.grid_width, self.grid_height, self.pit_prob, self.allow_climb_without_gold, \n",
    "                                      new_agent, self.pit_locations, death, self.wumpus_loc, self.wumpus_alive, new_gold_loc)\n",
    "                percept = Percept(new_env.is_stench(), new_env.is_breeze(), new_env.is_glitter(), \n",
    "                                  new_agent.location == self.agent.location, False, death, \n",
    "                                  -1 if new_agent.is_alive else -1001)\n",
    "                return (new_env, percept)\n",
    "            \n",
    "            if action.is_turn_left:\n",
    "                new_env = Environment(self.grid_width, self.grid_height, self.pit_prob, self.allow_climb_without_gold, \n",
    "                                      self.agent.turn_left(), self.pit_locations, self.terminated, self.wumpus_loc, \n",
    "                                      self.wumpus_alive, self.gold_loc)\n",
    "                percept = Percept(self.is_stench(), self.is_breeze(), self.is_glitter(), False, False, False, -1)\n",
    "                return (new_env, percept)\n",
    "            \n",
    "            if action.is_turn_right:\n",
    "                new_env = Environment(self.grid_width, self.grid_height, self.pit_prob, self.allow_climb_without_gold, \n",
    "                                      self.agent.turn_right(), self.pit_locations, self.terminated, self.wumpus_loc, \n",
    "                                      self.wumpus_alive, self.gold_loc)\n",
    "                percept = Percept(self.is_stench(), self.is_breeze(), self.is_glitter(), False, False, False, -1)\n",
    "                return (new_env, percept)\n",
    "            \n",
    "            if action.is_grab:\n",
    "                new_agent = copy.deepcopy(self.agent)\n",
    "                new_agent.has_gold = self.is_glitter()\n",
    "                new_gold_loc = new_agent.location if new_agent.has_gold else self.gold_loc\n",
    "                new_env = Environment(self.grid_width, self.grid_height, self.pit_prob, self.allow_climb_without_gold, \n",
    "                                      new_agent, self.pit_locations, self.terminated, self.wumpus_loc, self.wumpus_alive, \n",
    "                                      new_gold_loc)\n",
    "                percept = Percept(self.is_stench(), self.is_breeze(), self.is_glitter(), False, False, False, -1)\n",
    "                return (new_env, percept)\n",
    "            \n",
    "            if action.is_climb:\n",
    "                in_start_loc = self.agent.location == Coords(0, 0)\n",
    "                success = self.agent.has_gold and in_start_loc\n",
    "                is_terminated = success or (self.allow_climb_without_gold and in_start_loc)\n",
    "                new_env = Environment(self.grid_width, self.grid_height, self.pit_prob, self.allow_climb_without_gold, \n",
    "                                      self.agent, self.pit_locations, is_terminated, self.wumpus_loc, self.wumpus_alive, \n",
    "                                      self.gold_loc)\n",
    "                percept = Percept(self.is_stench(), self.is_breeze(), self.is_glitter(), False, False, is_terminated, \n",
    "                                  999 if success else -1)\n",
    "                return (new_env, percept)\n",
    "            \n",
    "            if action.is_shoot:\n",
    "                had_arrow = self.agent.has_arrow\n",
    "                wumpus_killed = self.kill_attempt_successful()\n",
    "                new_agent = copy.deepcopy(self.agent)\n",
    "                new_agent.has_arrow = False\n",
    "                new_env = Environment(self.grid_width, self.grid_height, self.pit_prob, self.allow_climb_without_gold, \n",
    "                                      new_agent, self.pit_locations, self.terminated, self.wumpus_loc, \n",
    "                                      self.wumpus_alive and (not wumpus_killed), self.gold_loc)\n",
    "                percept = Percept(self.is_stench(), self.is_breeze(), self.is_glitter(), False, wumpus_killed, False, \n",
    "                                  -11 if had_arrow else -1)\n",
    "                return (new_env, percept)\n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    def new_game(cls, grid_width, grid_height, pit_prob, allow_climb_without_gold):\n",
    "        new_pit_locations = create_pit_locations(grid_width, grid_height, pit_prob)\n",
    "        new_wumpus_loc = random_location_except_origin(grid_width, grid_height)\n",
    "        new_gold_loc = random_location_except_origin(grid_width, grid_height)\n",
    "        env = Environment(grid_width, grid_height, pit_prob, allow_climb_without_gold, \n",
    "                          AgentState(), new_pit_locations, False, new_wumpus_loc, True, new_gold_loc)\n",
    "        percept = Percept(env.is_stench(), env.is_breeze(), False, False, False, False, 0.0)\n",
    "        return (env, percept)\n",
    "    \n",
    "    \n",
    "    def visualize(self):\n",
    "        wumpus_symbol = \"W\" if self.wumpus_alive else \"w\"\n",
    "        all_rows = []\n",
    "        for y in range(self.grid_height - 1, -1, -1):\n",
    "            row = []\n",
    "            for x in range (self.grid_width):\n",
    "                agent = \"A\" if self.is_agent_at(Coords(x, y)) else \" \"\n",
    "                pit = \"P\" if self.is_pit_at(Coords(x, y)) else \" \"\n",
    "                gold = \"G\" if self.is_gold_at(Coords(x, y)) else \" \"\n",
    "                wumpus = wumpus_symbol if self.is_wumpus_at(Coords(x, y)) else \" \"\n",
    "                cell = agent + pit + gold + wumpus\n",
    "                row.append(cell)\n",
    "            row_str = \"|\".join(row)\n",
    "            all_rows.append(row_str)\n",
    "        final_str = \"\\n\".join(all_rows)\n",
    "        print(final_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functions to encode and decode actions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "jZ2MKdL4Z2Cu"
   },
   "outputs": [],
   "source": [
    "# Convert action to int\n",
    "\n",
    "def encode_action_to_int(action):\n",
    "    if action.is_forward:\n",
    "        action_int = 0\n",
    "    elif action.is_turn_left:\n",
    "        action_int = 1\n",
    "    elif action.is_turn_right:\n",
    "        action_int = 2\n",
    "    elif action.is_shoot:\n",
    "        action_int = 3\n",
    "    elif action.is_grab:\n",
    "        action_int = 4\n",
    "    else: # climb\n",
    "        action_int = 5\n",
    "    return action_int\n",
    "\n",
    "\n",
    "\n",
    "# Convert action index (int) to action\n",
    "\n",
    "def decode_action_index(index):\n",
    "    actions = [Action.forward(), Action.turn_left(), Action.turn_right(), Action.shoot(), Action.grab(), Action.climb()]\n",
    "    return actions[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The `ExperienceBuffer` and `ExperienceCollector` classes: for handling experience data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "-pnJdxPlZ2Cu"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# The ExperienceBuffer class to store the states, actions and rewards as NumPy arrays\n",
    "\n",
    "class ExperienceBuffer:\n",
    "    def __init__(self, states, actions, rewards):\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.rewards = rewards\n",
    "    \n",
    "    def serialize(self, h5file):\n",
    "        h5file.create_group('experience')\n",
    "        h5file['experience'].create_dataset('states', data=self.states)\n",
    "        h5file['experience'].create_dataset('actions', data=self.actions)\n",
    "        h5file['experience'].create_dataset('rewards', data=self.rewards)\n",
    "\n",
    "\n",
    "\n",
    "# Function to load the experience buffer from HDF5 file\n",
    "\n",
    "def load_experience(h5file):\n",
    "    return ExperienceBuffer(\n",
    "        states=np.array(h5file['experience']['states']),\n",
    "        actions=np.array(h5file['experience']['actions']),\n",
    "        rewards=np.array(h5file['experience']['rewards']))\n",
    "\n",
    "\n",
    "\n",
    "# The ExperienceCollector class to collect all the states, decisions and rewards (as Python lists)\n",
    "\n",
    "class ExperienceCollector:\n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "    \n",
    "    def record_state(self, state):\n",
    "        self.states.append(state)\n",
    "    \n",
    "    def record_action(self, action):\n",
    "        self.actions.append(action)\n",
    "    \n",
    "    def record_reward(self, reward):\n",
    "        self.rewards.append(reward)\n",
    "    \n",
    "    def to_buffer(self):\n",
    "        return ExperienceBuffer(\n",
    "            states=np.array(self.states), \n",
    "            actions=np.array(self.actions), \n",
    "            rewards=np.array(self.rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "BihayXJJZ2Cu"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def select_action(self, percept):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The `DeepQAgent` class: a Q-learning agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Ist8ZWLrZ2Cu"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "\n",
    "class DeepQAgent(Agent):\n",
    "    def __init__(self, model, grid_width, grid_height, agent_state,\n",
    "                 visited_locations, stench_locations, breeze_locations, \n",
    "                 perceives_glitter, heard_scream, perceives_bump):\n",
    "        self.model = model\n",
    "        self.grid_width = grid_width\n",
    "        self.grid_height = grid_height\n",
    "        self.agent_state = agent_state\n",
    "        self.visited_locations = set(visited_locations)\n",
    "        self.stench_locations = set(stench_locations)\n",
    "        self.breeze_locations = set(breeze_locations)\n",
    "        self.perceives_glitter = perceives_glitter\n",
    "        self.heard_scream = heard_scream\n",
    "        self.perceives_bump = perceives_bump\n",
    "        self.epsilon = 0.0\n",
    "        self.collector = None\n",
    "        \n",
    "    \n",
    "    \n",
    "    # Control the epsilon-greedy policy\n",
    "    def set_epsilon(self, epsilon):\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Attach an ExperienceCollector object to record the experience data\n",
    "    def set_collector(self, collector):\n",
    "        self.collector = collector\n",
    "    \n",
    "    \n",
    "    \n",
    "    def select_action(self, percept):\n",
    "        \n",
    "        # Update agent's variables\n",
    "        visiting_new_location = self.agent_state.location not in self.visited_locations\n",
    "        if visiting_new_location:\n",
    "            self.visited_locations.add(self.agent_state.location)\n",
    "        if percept.breeze:\n",
    "            self.breeze_locations.add(self.agent_state.location)\n",
    "        if percept.stench:\n",
    "            self.stench_locations.add(self.agent_state.location)\n",
    "        new_heard_scream = self.heard_scream or percept.scream\n",
    "        self.heard_scream = new_heard_scream\n",
    "        self.perceives_glitter = percept.glitter\n",
    "        self.perceives_bump = percept.bump\n",
    "        \n",
    "        num_actions = 6\n",
    "        \n",
    "        state_tensor = self.encode_belief_state() # encode belief state\n",
    "        state_tensors_list = [state_tensor for i in range(num_actions)] # list with 6 state tensors (the same items)\n",
    "        state_tensors_array = np.array(state_tensors_list)\n",
    "        \n",
    "        # One-hot encode all 6 actions\n",
    "        action_vectors = np.zeros((num_actions, num_actions))\n",
    "        for i in range(num_actions):\n",
    "            action_vectors[i][i] = 1\n",
    "        \n",
    "        # Predict values (using two inputs)\n",
    "        values = self.model.predict([state_tensors_array, action_vectors])\n",
    "        values = values.reshape(num_actions) # convert a matrix to a vector\n",
    "        ranked_moves = self.rank_moves_eps_greedy(values) # rank the actions according to the epsilon-greedy policy\n",
    "        action_index = ranked_moves[0] # index of the largest value\n",
    "        if self.collector is not None: # record the state and decision if collecting experience\n",
    "            self.collector.record_state(state=state_tensor)\n",
    "            self.collector.record_action(action_index)\n",
    "        next_action = decode_action_index(action_index) # decode the action from index\n",
    "\n",
    "        if next_action.is_grab:\n",
    "            if percept.glitter and not self.agent_state.has_gold:\n",
    "                self.agent_state.has_gold = True\n",
    "        else:\n",
    "            self.agent_state = self.agent_state.apply_move_action(next_action, self.grid_width, self.grid_height)\n",
    "        return (self, next_action)\n",
    "    \n",
    "    \n",
    "\n",
    "    def rank_moves_eps_greedy(self, values):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            values = np.random.random(values.shape)\n",
    "        ranked_moves = np.argsort(values) # rank the actions from worst to best\n",
    "        # Return actions in best-to-worst order (a reversed vector)\n",
    "        return ranked_moves[::-1]\n",
    "    \n",
    "    \n",
    "    \n",
    "    def train(self, experience, lr=0.01, batch_size=128, epochs=1):\n",
    "        opt = keras.optimizers.Adam(lr=lr)\n",
    "        self.model.compile(loss='mse', optimizer=opt)\n",
    "\n",
    "        n = experience.states.shape[0] # number of experience samples\n",
    "        num_actions = 6\n",
    "        y = np.zeros((n,)) # the target vector with rewards\n",
    "        actions = np.zeros((n, num_actions))\n",
    "        for i in range(n):\n",
    "            action = experience.actions[i]\n",
    "            reward = experience.rewards[i]\n",
    "            actions[i][action] = 1 # one_hot encode actions\n",
    "            y[i] = reward\n",
    "\n",
    "        self.model.fit([experience.states, actions], y, batch_size=batch_size, epochs=epochs)\n",
    "    \n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    def new_agent(cls, model, grid_width, grid_height):\n",
    "        return DeepQAgent(model, grid_width, grid_height, AgentState(), set(), set(), set(), False, False, False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Encode belief state using 13 feature planes (each plane is a grid_height x grid_width matrix)\n",
    "    # The state shape is (13, grid_height, grid_width)\n",
    "    \n",
    "    def encode_belief_state(self):\n",
    "        state_tensor = np.zeros((13, self.grid_height, self.grid_width)) # create a 3-D tensor\n",
    "        all_cells = list_all_locations(self.grid_width, self.grid_height)\n",
    "        \n",
    "        # The first plane has a 1 for agent's location and 0s for other locations\n",
    "        state_tensor[0][self.agent_state.location.y][self.agent_state.location.x] = 1\n",
    "        \n",
    "        for cell in all_cells:\n",
    "            if cell in self.visited_locations:\n",
    "                state_tensor[1][cell.y][cell.x] = 1 # 1s for visited locations\n",
    "            if cell in self.stench_locations:\n",
    "                state_tensor[2][cell.y][cell.x] = 1 # 1s for stench locations\n",
    "            if cell in self.breeze_locations:\n",
    "                state_tensor[3][cell.y][cell.x] = 1 # 1s for breeze locations\n",
    "        \n",
    "        if self.agent_state.orientation == Orientation.north: # a plane filled with 1s if Orientation.north\n",
    "            state_tensor[4] = 1\n",
    "        elif self.agent_state.orientation == Orientation.south: # a plane filled with 1s if Orientation.south\n",
    "            state_tensor[5] = 1\n",
    "        elif self.agent_state.orientation == Orientation.east: # a plane filled with 1s if Orientation.east\n",
    "            state_tensor[6] = 1\n",
    "        else: # a plane filled with 1s if Orientation.west\n",
    "            state_tensor[7] = 1\n",
    "        \n",
    "        if self.agent_state.has_gold: # a plane filled with 1s if agent has gold, and 0s otherwise\n",
    "            state_tensor[8] = 1\n",
    "        if self.perceives_glitter: # a plane filled with 1s if agent perceives glitter, and 0s otherwise\n",
    "            state_tensor[9] = 1\n",
    "        if self.agent_state.has_arrow: # a plane filled with 1s if agent has arrow, and 0s otherwise\n",
    "            state_tensor[10] = 1\n",
    "        if self.heard_scream: # a plane filled with 1s if wumpus is not alive, and 0s otherwise\n",
    "            state_tensor[11] = 1\n",
    "        if self.perceives_bump: # a plane filled with 1s if agent perceives bump, and 0s otherwise\n",
    "            state_tensor[12] = 1\n",
    "        \n",
    "        return state_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function to create an action-value network with two inputs (states and actions) and one output (the action-value)**\n",
    "\n",
    "The encoded state goes through several convolutional layers. The proposed action goes into a separate input. The output of the convolutional layers is combined with the proposed action and passed through a dense layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "T_BHG79bZ2Cv"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, concatenate\n",
    "from tensorflow.keras.layers import ZeroPadding2D, Conv2D, BatchNormalization, Activation\n",
    "\n",
    "\n",
    "def create_action_value_network(state_shape):\n",
    "    state_input = Input(shape=state_shape, name='state_input')\n",
    "    action_input = Input(shape=(6,), name='action_input')\n",
    "    \n",
    "    conv1a = ZeroPadding2D((1, 1), data_format='channels_first')(state_input)\n",
    "    conv1b = Conv2D(64, (3, 3), data_format='channels_first')(conv1a)\n",
    "    conv1c = BatchNormalization(axis=1)(conv1b)\n",
    "    conv1d = Activation('relu')(conv1c)\n",
    "    \n",
    "    conv2a = ZeroPadding2D((1, 1), data_format='channels_first')(conv1d)\n",
    "    conv2b = Conv2D(64, (3, 3), data_format='channels_first')(conv2a)\n",
    "    conv2c = BatchNormalization(axis=1)(conv2b)\n",
    "    conv2d = Activation('relu')(conv2c)\n",
    "    \n",
    "    conv3a = ZeroPadding2D((1, 1), data_format='channels_first')(conv2d)\n",
    "    conv3b = Conv2D(48, (3, 3), data_format='channels_first')(conv3a)\n",
    "    conv3c = BatchNormalization(axis=1)(conv3b)\n",
    "    conv3d = Activation('relu')(conv3c)\n",
    "\n",
    "    conv4a = ZeroPadding2D((1, 1), data_format='channels_first')(conv3d)\n",
    "    conv4b = Conv2D(32, (3, 3), data_format='channels_first')(conv4a)\n",
    "    conv4c = BatchNormalization(axis=1)(conv4b)\n",
    "    conv4d = Activation('relu')(conv4c)\n",
    "    \n",
    "    flat = Flatten()(conv4d)\n",
    "    processed_state = Dense(512, activation='relu')(flat)\n",
    "    \n",
    "    state_and_action = concatenate([action_input, processed_state])\n",
    "    hidden_layer = Dense(256, activation='relu')(state_and_action)\n",
    "    value_output = Dense(1)(hidden_layer)\n",
    "    \n",
    "    model = Model(inputs=[state_input, action_input], outputs=value_output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting experience, training and evaluating the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W02cch8alFDk",
    "outputId": "de9c3007-6ff3-4a6a-a86b-2122914f05a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train the DeepQAgent on the experience data generated by the probabilistic agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wCj1DMy3fAka",
    "outputId": "4bc20bd4-8c3f-4a0a-ea47-9977b237e12c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76528, 13, 4, 4)"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the experience data generated by the probabilistic agent (5,000 games)\n",
    "\n",
    "import h5py\n",
    "\n",
    "prob_agent_experience_02 = load_experience(h5py.File('drive/My Drive/prob_agent_experience_02', 'r'))\n",
    "prob_agent_experience_02.states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ufMl384OpESC",
    "outputId": "5c031101-0ce5-44a4-cdbf-3374e86cd343"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "state_input (InputLayer)        [(None, 13, 4, 4)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2D)  (None, 13, 6, 6)     0           state_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 64, 4, 4)     7552        zero_padding2d[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 64, 4, 4)     256         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 64, 4, 4)     0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D (None, 64, 6, 6)     0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 64, 4, 4)     36928       zero_padding2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 64, 4, 4)     256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 64, 4, 4)     0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPadding2D (None, 64, 6, 6)     0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 48, 4, 4)     27696       zero_padding2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 48, 4, 4)     192         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 48, 4, 4)     0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPadding2D (None, 48, 6, 6)     0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 4, 4)     13856       zero_padding2d_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 4, 4)     128         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 4, 4)     0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 512)          0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "action_input (InputLayer)       [(None, 6)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          262656      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 518)          0           action_input[0][0]               \n",
      "                                                                 dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          132864      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            257         dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 482,641\n",
      "Trainable params: 482,225\n",
      "Non-trainable params: 416\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create an action-value network\n",
    "\n",
    "model_6_2 = create_action_value_network(prob_agent_experience_02.states[0].shape)\n",
    "model_6_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ArrBhUlRps6_",
    "outputId": "31ad5250-625d-4dba-df01-fb8dc3e8f4a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 10213.8916\n",
      "Epoch 2/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 7217.5479\n",
      "Epoch 3/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 6507.2002\n",
      "Epoch 4/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 6280.7847\n",
      "Epoch 5/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 6016.0669\n",
      "Epoch 6/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 6490.6445\n",
      "Epoch 7/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5881.6895\n",
      "Epoch 8/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5907.8770\n",
      "Epoch 9/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5931.5537\n",
      "Epoch 10/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 6108.4575\n",
      "Epoch 11/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5861.1504\n",
      "Epoch 12/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5843.1694\n",
      "Epoch 13/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5847.5449\n",
      "Epoch 14/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5792.9849\n",
      "Epoch 15/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5859.0044\n",
      "Epoch 16/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5775.2266\n",
      "Epoch 17/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5786.0161\n",
      "Epoch 18/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5762.4912\n",
      "Epoch 19/200\n",
      "598/598 [==============================] - 2s 4ms/step - loss: 5768.2290\n",
      "Epoch 20/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5923.1206\n",
      "Epoch 21/200\n",
      "598/598 [==============================] - 2s 4ms/step - loss: 5742.0796\n",
      "Epoch 22/200\n",
      "598/598 [==============================] - 2s 4ms/step - loss: 5702.0562\n",
      "Epoch 23/200\n",
      "598/598 [==============================] - 2s 4ms/step - loss: 5728.5181\n",
      "Epoch 24/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5766.8223\n",
      "Epoch 25/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5682.0859\n",
      "Epoch 26/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5825.6060\n",
      "Epoch 27/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5726.8022\n",
      "Epoch 28/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5713.7280\n",
      "Epoch 29/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5710.5039\n",
      "Epoch 30/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5687.0952\n",
      "Epoch 31/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5687.6494\n",
      "Epoch 32/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5683.4136\n",
      "Epoch 33/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5801.7529\n",
      "Epoch 34/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5686.4263\n",
      "Epoch 35/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5716.3828\n",
      "Epoch 36/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5648.2856\n",
      "Epoch 37/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5626.9038\n",
      "Epoch 38/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5670.7930\n",
      "Epoch 39/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5726.3218\n",
      "Epoch 40/200\n",
      "598/598 [==============================] - 2s 4ms/step - loss: 5697.1919\n",
      "Epoch 41/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5650.2974\n",
      "Epoch 42/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5625.8677\n",
      "Epoch 43/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5634.7046\n",
      "Epoch 44/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5617.8364\n",
      "Epoch 45/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5602.1826\n",
      "Epoch 46/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5631.7280\n",
      "Epoch 47/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5556.1587\n",
      "Epoch 48/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5593.7139\n",
      "Epoch 49/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5612.7129\n",
      "Epoch 50/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5570.5498\n",
      "Epoch 51/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5578.7207\n",
      "Epoch 52/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5580.4766\n",
      "Epoch 53/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5475.8765\n",
      "Epoch 54/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5515.8984\n",
      "Epoch 55/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5540.9995\n",
      "Epoch 56/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5574.3540\n",
      "Epoch 57/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5422.0498\n",
      "Epoch 58/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5407.4893\n",
      "Epoch 59/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5433.0732\n",
      "Epoch 60/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5458.9766\n",
      "Epoch 61/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5414.6855\n",
      "Epoch 62/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5400.2563\n",
      "Epoch 63/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5408.6191\n",
      "Epoch 64/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5379.1709\n",
      "Epoch 65/200\n",
      "598/598 [==============================] - 2s 4ms/step - loss: 5330.5684\n",
      "Epoch 66/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5353.6377\n",
      "Epoch 67/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5263.3354\n",
      "Epoch 68/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5323.5142\n",
      "Epoch 69/200\n",
      "598/598 [==============================] - 2s 4ms/step - loss: 5254.1738\n",
      "Epoch 70/200\n",
      "598/598 [==============================] - 2s 4ms/step - loss: 5203.5386\n",
      "Epoch 71/200\n",
      "598/598 [==============================] - 2s 4ms/step - loss: 5222.8169\n",
      "Epoch 72/200\n",
      "598/598 [==============================] - 2s 4ms/step - loss: 5192.4121\n",
      "Epoch 73/200\n",
      "598/598 [==============================] - 2s 4ms/step - loss: 5222.2363\n",
      "Epoch 74/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5204.1929\n",
      "Epoch 75/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5176.8052\n",
      "Epoch 76/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5154.0117\n",
      "Epoch 77/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5131.8516\n",
      "Epoch 78/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5135.3691\n",
      "Epoch 79/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5118.6738\n",
      "Epoch 80/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5174.4062\n",
      "Epoch 81/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5129.2349\n",
      "Epoch 82/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5047.6401\n",
      "Epoch 83/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5129.8604\n",
      "Epoch 84/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5105.7417\n",
      "Epoch 85/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5029.2886\n",
      "Epoch 86/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5087.6440\n",
      "Epoch 87/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5096.4028\n",
      "Epoch 88/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5091.7427\n",
      "Epoch 89/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4987.2231\n",
      "Epoch 90/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5011.6802\n",
      "Epoch 91/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5011.8633\n",
      "Epoch 92/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4927.3911\n",
      "Epoch 93/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 5015.0400\n",
      "Epoch 94/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4988.1084\n",
      "Epoch 95/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4926.4165\n",
      "Epoch 96/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4985.0249\n",
      "Epoch 97/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4889.3193\n",
      "Epoch 98/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4901.3218\n",
      "Epoch 99/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4871.6035\n",
      "Epoch 100/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4883.4946\n",
      "Epoch 101/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4871.7397\n",
      "Epoch 102/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4885.6104\n",
      "Epoch 103/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4841.2344\n",
      "Epoch 104/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4850.8516\n",
      "Epoch 105/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4870.2109\n",
      "Epoch 106/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4992.3008\n",
      "Epoch 107/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4843.0913\n",
      "Epoch 108/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4774.5352\n",
      "Epoch 109/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4843.0039\n",
      "Epoch 110/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4804.8613\n",
      "Epoch 111/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4793.4492\n",
      "Epoch 112/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4891.1348\n",
      "Epoch 113/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4808.4307\n",
      "Epoch 114/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4854.1641\n",
      "Epoch 115/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4738.7686\n",
      "Epoch 116/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4815.3462\n",
      "Epoch 117/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4743.2842\n",
      "Epoch 118/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4856.3530\n",
      "Epoch 119/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4767.8809\n",
      "Epoch 120/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4809.2925\n",
      "Epoch 121/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4677.8374\n",
      "Epoch 122/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4722.9814\n",
      "Epoch 123/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4775.5601\n",
      "Epoch 124/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4862.8506\n",
      "Epoch 125/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4765.5093\n",
      "Epoch 126/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4695.8037\n",
      "Epoch 127/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4702.0039\n",
      "Epoch 128/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4641.9727\n",
      "Epoch 129/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4722.2339\n",
      "Epoch 130/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4679.8794\n",
      "Epoch 131/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4674.4482\n",
      "Epoch 132/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4686.4150\n",
      "Epoch 133/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4652.9971\n",
      "Epoch 134/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4658.1616\n",
      "Epoch 135/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4647.8481\n",
      "Epoch 136/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4709.7227\n",
      "Epoch 137/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4753.1108\n",
      "Epoch 138/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4733.8291\n",
      "Epoch 139/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4631.2358\n",
      "Epoch 140/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4694.6709\n",
      "Epoch 141/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4640.4697\n",
      "Epoch 142/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4669.9434\n",
      "Epoch 143/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4613.9238\n",
      "Epoch 144/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4586.5352\n",
      "Epoch 145/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4584.2671\n",
      "Epoch 146/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4693.2734\n",
      "Epoch 147/200\n",
      "598/598 [==============================] - 2s 4ms/step - loss: 4824.0713\n",
      "Epoch 148/200\n",
      "598/598 [==============================] - 2s 4ms/step - loss: 4698.8730\n",
      "Epoch 149/200\n",
      "598/598 [==============================] - 2s 4ms/step - loss: 4589.5845\n",
      "Epoch 150/200\n",
      "598/598 [==============================] - 2s 4ms/step - loss: 4596.6826\n",
      "Epoch 151/200\n",
      "598/598 [==============================] - 2s 4ms/step - loss: 4658.0068\n",
      "Epoch 152/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4636.8486\n",
      "Epoch 153/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4573.0596\n",
      "Epoch 154/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4615.4409\n",
      "Epoch 155/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4548.2568\n",
      "Epoch 156/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4589.2144\n",
      "Epoch 157/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4593.0742\n",
      "Epoch 158/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4678.7759\n",
      "Epoch 159/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4626.4512\n",
      "Epoch 160/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4607.4233\n",
      "Epoch 161/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4518.2749\n",
      "Epoch 162/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4555.7363\n",
      "Epoch 163/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4617.7012\n",
      "Epoch 164/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4580.6938\n",
      "Epoch 165/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4605.2427\n",
      "Epoch 166/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4618.4487\n",
      "Epoch 167/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4617.9624\n",
      "Epoch 168/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4538.0327\n",
      "Epoch 169/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4577.2202\n",
      "Epoch 170/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4521.0503\n",
      "Epoch 171/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4494.4717\n",
      "Epoch 172/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4549.2300\n",
      "Epoch 173/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4544.4116\n",
      "Epoch 174/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4504.8584\n",
      "Epoch 175/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4509.3662\n",
      "Epoch 176/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4484.9360\n",
      "Epoch 177/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4510.5400\n",
      "Epoch 178/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4501.9937\n",
      "Epoch 179/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4474.4180\n",
      "Epoch 180/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4615.6348\n",
      "Epoch 181/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4528.1685\n",
      "Epoch 182/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4517.0552\n",
      "Epoch 183/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4530.7720\n",
      "Epoch 184/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4553.9653\n",
      "Epoch 185/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4461.7251\n",
      "Epoch 186/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4507.9722\n",
      "Epoch 187/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4475.6323\n",
      "Epoch 188/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4599.4702\n",
      "Epoch 189/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4500.2183\n",
      "Epoch 190/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4462.5347\n",
      "Epoch 191/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4476.7148\n",
      "Epoch 192/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4521.7686\n",
      "Epoch 193/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4496.1738\n",
      "Epoch 194/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4477.7783\n",
      "Epoch 195/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4457.7646\n",
      "Epoch 196/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4468.9702\n",
      "Epoch 197/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4449.8750\n",
      "Epoch 198/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4488.8511\n",
      "Epoch 199/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4473.9751\n",
      "Epoch 200/200\n",
      "598/598 [==============================] - 2s 3ms/step - loss: 4524.3003\n"
     ]
    }
   ],
   "source": [
    "# Create a DeepQAgent (4x4 grid) and train it from the experience data\n",
    "# Adam optimizer, lr=0.01, epochs=200\n",
    "\n",
    "agent_6_2 = DeepQAgent.new_agent(model_6_2, 4, 4)\n",
    "agent_6_2.train(prob_agent_experience_02, lr=0.01, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7oW_usqSqkGb",
    "outputId": "63e636ae-439e-4cd3-a974-bf9ab8a79d7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: drive/My Drive/model_6_2/assets\n"
     ]
    }
   ],
   "source": [
    "agent_6_2.model.save('drive/My Drive/model_6_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "APtT2Cj_rH7z",
    "outputId": "2566b70a-3356-4d56-a1d1-573e0a762c0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "state_input (InputLayer)        [(None, 13, 4, 4)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2D)  (None, 13, 6, 6)     0           state_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 64, 4, 4)     7552        zero_padding2d[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 64, 4, 4)     256         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 64, 4, 4)     0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D (None, 64, 6, 6)     0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 64, 4, 4)     36928       zero_padding2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 64, 4, 4)     256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 64, 4, 4)     0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPadding2D (None, 64, 6, 6)     0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 48, 4, 4)     27696       zero_padding2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 48, 4, 4)     192         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 48, 4, 4)     0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPadding2D (None, 48, 6, 6)     0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 4, 4)     13856       zero_padding2d_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 4, 4)     128         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 4, 4)     0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 512)          0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "action_input (InputLayer)       [(None, 6)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          262656      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 518)          0           action_input[0][0]               \n",
      "                                                                 dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          132864      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            257         dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 482,641\n",
      "Trainable params: 482,225\n",
      "Non-trainable params: 416\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model('drive/My Drive/model_6_2')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluating the updated agent: running 1,000 games and calculating the average score per game**\n",
    "\n",
    "Only if the agent climbs out with gold, it is counted as a win."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BlByQRTErxnx",
    "outputId": "97e49834-b40c-48b6-ad51-1c23f4670cb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsilon = 0.0\n",
      "Number of games: 10\n",
      "Total number of moves: 10\n",
      "n_games_reward: -10\n",
      "avg_reward_per_game: -1\n",
      "wins/games: 0.000\n",
      "games_stopped: 0\n",
      "rewards_list: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n"
     ]
    }
   ],
   "source": [
    "# If epsilon is set to 0.0, the agent climbs out without gold during the first step\n",
    "\n",
    "n_games = 10\n",
    "total_moves = 0\n",
    "n_games_reward = 0\n",
    "rewards_list = []\n",
    "wins = 0\n",
    "games_stopped = 0\n",
    "\n",
    "for i in range(n_games):\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(\"Game %d/%d ...\" % (i + 1, n_games))\n",
    "    \n",
    "    agent = DeepQAgent.new_agent(model, 4, 4)\n",
    "    (env, percept) = Environment.new_game(4, 4, 0.2, True)\n",
    "    #env.visualize()\n",
    "    #percept.show()\n",
    "    total_reward = 0\n",
    "    num_moves = 0\n",
    "\n",
    "    #agent.set_epsilon(0.5)\n",
    "    \n",
    "    while not percept.is_terminated:\n",
    "        (agent, next_action) = agent.select_action(percept)\n",
    "        #print(\"next_action:\", next_action.show())\n",
    "        #agent.agent_state.show()\n",
    "\n",
    "        (env, percept) = env.apply_action(next_action)\n",
    "        #env.visualize()\n",
    "        #percept.show()\n",
    "        total_reward += percept.reward\n",
    "        num_moves += 1\n",
    "        if num_moves > 199:\n",
    "            games_stopped += 1\n",
    "            break\n",
    "    \n",
    "    if total_reward > 0: # only if the agent climbs out with gold, it is counted as a win\n",
    "        wins += 1\n",
    "    n_games_reward += total_reward\n",
    "    total_moves += num_moves\n",
    "    rewards_list.append(total_reward)\n",
    "\n",
    "print(\"epsilon =\", agent.epsilon)\n",
    "print(\"Number of games:\", n_games)\n",
    "print(\"Total number of moves:\", total_moves)\n",
    "print(\"n_games_reward:\", n_games_reward)\n",
    "print(\"avg_reward_per_game: %.f\" % (n_games_reward / n_games))\n",
    "print(\"wins/games: %.3f\" % (wins / n_games))\n",
    "print(\"games_stopped:\", games_stopped)\n",
    "print(\"rewards_list:\", rewards_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wOIO_c0CswJN",
    "outputId": "46a98444-9732-4d4a-946c-3e26b78ae5ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 100/1000 ...\n",
      "Game 200/1000 ...\n",
      "Game 300/1000 ...\n",
      "Game 400/1000 ...\n",
      "Game 500/1000 ...\n",
      "Game 600/1000 ...\n",
      "Game 700/1000 ...\n",
      "Game 800/1000 ...\n",
      "Game 900/1000 ...\n",
      "Game 1000/1000 ...\n",
      "epsilon = 0.5\n",
      "Number of games: 1000\n",
      "Total number of moves: 6457\n",
      "n_games_reward: -90057\n",
      "avg_reward_per_game: -90\n",
      "wins/games: 0.001\n",
      "games_stopped: 7\n",
      "rewards_list[:100]: [-1, -1, -1, -1039, -12, -1, -2, -12, -1, -2, -1, -1, -3, -1, -1, -1, -1, -1, -13, -1, -1, -3, -1, -1, -1, -1, -2, -12, -12, -1, -3, -1, -1, -1, -3, -71, -1, -3, -2, -2, -3, -2, -1, -1, -1, -1, -1, -1, -2, -12, -1, -1, -2, -1, -2, -1, -2, -1, -18, -1, -1, -1, -1, -1, -12, -1, -3, -1, -1, -1, -2, -1, -1, -1, -1, -2, -1001, -2, -1, -12, -2, -1, -1, -3, -1196, -1, -1, -1108, -1112, -74, -2, -1, -2, -13, -2, -1, -2, -1087, -1, -1]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the updated agent (1,000 games)\n",
    "\n",
    "# epsilon = 0.5\n",
    "# Average total reward per game = -90\n",
    "# wins/games: 0.001\n",
    "# games_stopped: 7\n",
    "\n",
    "\n",
    "n_games = 1000\n",
    "total_moves = 0\n",
    "n_games_reward = 0\n",
    "rewards_list = []\n",
    "wins = 0\n",
    "games_stopped = 0\n",
    "\n",
    "for i in range(n_games):\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(\"Game %d/%d ...\" % (i + 1, n_games))\n",
    "    \n",
    "    agent = DeepQAgent.new_agent(model, 4, 4)\n",
    "    (env, percept) = Environment.new_game(4, 4, 0.2, True)\n",
    "    #env.visualize()\n",
    "    #percept.show()\n",
    "    total_reward = 0\n",
    "    num_moves = 0\n",
    "\n",
    "    agent.set_epsilon(0.5)\n",
    "    \n",
    "    while not percept.is_terminated:\n",
    "        (agent, next_action) = agent.select_action(percept)\n",
    "        #print(\"next_action:\", next_action.show())\n",
    "        #agent.agent_state.show()\n",
    "\n",
    "        (env, percept) = env.apply_action(next_action)\n",
    "        #env.visualize()\n",
    "        #percept.show()\n",
    "        total_reward += percept.reward\n",
    "        num_moves += 1\n",
    "        if num_moves > 199:\n",
    "            games_stopped += 1\n",
    "            break\n",
    "    \n",
    "    if total_reward > 0:\n",
    "        wins += 1\n",
    "    n_games_reward += total_reward\n",
    "    total_moves += num_moves\n",
    "    rewards_list.append(total_reward)\n",
    "\n",
    "print(\"epsilon =\", agent.epsilon)\n",
    "print(\"Number of games:\", n_games)\n",
    "print(\"Total number of moves:\", total_moves)\n",
    "print(\"n_games_reward:\", n_games_reward)\n",
    "print(\"avg_reward_per_game: %.f\" % (n_games_reward / n_games))\n",
    "print(\"wins/games: %.3f\" % (wins / n_games))\n",
    "print(\"games_stopped:\", games_stopped)\n",
    "print(\"rewards_list[:100]:\", rewards_list[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LrfZtidKFTXi"
   },
   "source": [
    "**Collect new experience data using the updated DeepQAgent and save it to file (5,000 games)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iJoMuPABFTLY",
    "outputId": "a54438aa-48e5-4831-f763-887e40abc4b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 100/5000 ...\n",
      "Game 200/5000 ...\n",
      "Game 300/5000 ...\n",
      "Game 400/5000 ...\n",
      "Game 500/5000 ...\n",
      "Game 600/5000 ...\n",
      "Game 700/5000 ...\n",
      "Game 800/5000 ...\n",
      "Game 900/5000 ...\n",
      "Game 1000/5000 ...\n",
      "Game 1100/5000 ...\n",
      "Game 1200/5000 ...\n",
      "Game 1300/5000 ...\n",
      "Game 1400/5000 ...\n",
      "Game 1500/5000 ...\n",
      "Game 1600/5000 ...\n",
      "Game 1700/5000 ...\n",
      "Game 1800/5000 ...\n",
      "Game 1900/5000 ...\n",
      "Game 2000/5000 ...\n",
      "Game 2100/5000 ...\n",
      "Game 2200/5000 ...\n",
      "Game 2300/5000 ...\n",
      "Game 2400/5000 ...\n",
      "Game 2500/5000 ...\n",
      "Game 2600/5000 ...\n",
      "Game 2700/5000 ...\n",
      "Game 2800/5000 ...\n",
      "Game 2900/5000 ...\n",
      "Game 3000/5000 ...\n",
      "Game 3100/5000 ...\n",
      "Game 3200/5000 ...\n",
      "Game 3300/5000 ...\n",
      "Game 3400/5000 ...\n",
      "Game 3500/5000 ...\n",
      "Game 3600/5000 ...\n",
      "Game 3700/5000 ...\n",
      "Game 3800/5000 ...\n",
      "Game 3900/5000 ...\n",
      "Game 4000/5000 ...\n",
      "Game 4100/5000 ...\n",
      "Game 4200/5000 ...\n",
      "Game 4300/5000 ...\n",
      "Game 4400/5000 ...\n",
      "Game 4500/5000 ...\n",
      "Game 4600/5000 ...\n",
      "Game 4700/5000 ...\n",
      "Game 4800/5000 ...\n",
      "Game 4900/5000 ...\n",
      "Game 5000/5000 ...\n",
      "Number of games: 5000\n",
      "Total number of moves: 31416\n",
      "n_games_reward: -414996\n",
      "avg_reward_per_game: -83\n",
      "wins/games: 0.002\n",
      "games_stopped: 36\n",
      "exp.states.shape: (31416, 13, 4, 4)\n",
      "exp.actions.shape: (31416,)\n",
      "exp.rewards.shape: (31416,)\n",
      "exp.states[0]: [[[1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]]\n",
      "exp.actions[0]: 5\n",
      "exp.rewards[0]: -1\n"
     ]
    }
   ],
   "source": [
    "# 5,000 games\n",
    "# epsilon = 0.5\n",
    "# Average total reward per game = -83\n",
    "# Total number of moves: 31,416\n",
    "# wins/games: 0.002\n",
    "# games_stopped: 36 (after 200 moves)\n",
    "\n",
    "\n",
    "import h5py\n",
    "\n",
    "\n",
    "n_games = 5000\n",
    "total_moves = 0\n",
    "n_games_reward = 0\n",
    "wins = 0\n",
    "\n",
    "collector = ExperienceCollector()\n",
    "\n",
    "for i in range(n_games):\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(\"Game %d/%d ...\" % (i + 1, n_games))\n",
    "    \n",
    "    agent = DeepQAgent.new_agent(model, 4, 4)\n",
    "    (env, percept) = Environment.new_game(4, 4, 0.2, True)\n",
    "    total_reward = 0\n",
    "    num_moves = 0\n",
    "    \n",
    "    agent.set_epsilon(0.5)\n",
    "\n",
    "    while not percept.is_terminated:\n",
    "        agent.set_collector(collector)\n",
    "        (agent, next_action) = agent.select_action(percept)\n",
    "        (env, percept) = env.apply_action(next_action)\n",
    "        collector.record_reward(percept.reward) # add reward to collector\n",
    "        total_reward += percept.reward\n",
    "        num_moves += 1\n",
    "        if num_moves > 199:\n",
    "            games_stopped += 1\n",
    "            break\n",
    "        \n",
    "        #print(\"Game %d/%d\" % (i + 1, n_games))\n",
    "        #print(\"Total reward:\", total_reward)\n",
    "        #print(\"Moves per episode:\", num_moves)\n",
    "    \n",
    "    if total_reward > 0:\n",
    "        wins += 1\n",
    "    n_games_reward += total_reward\n",
    "    total_moves += num_moves\n",
    "\n",
    "print(\"Number of games:\", n_games)\n",
    "print(\"Total number of moves:\", total_moves)\n",
    "print(\"n_games_reward:\", n_games_reward)\n",
    "print(\"avg_reward_per_game: %.f\" % (n_games_reward / n_games))\n",
    "print(\"wins/games: %.3f\" % (wins / n_games))\n",
    "print(\"games_stopped:\", games_stopped)\n",
    "\n",
    "experience = collector.to_buffer()\n",
    "with h5py.File('drive/My Drive/q_agent_experience_6_2_1', 'w') as exp_out:\n",
    "    experience.serialize(exp_out)\n",
    "\n",
    "print(\"exp.states.shape:\", experience.states.shape)\n",
    "print(\"exp.actions.shape:\", experience.actions.shape)\n",
    "print(\"exp.rewards.shape:\", experience.rewards.shape)\n",
    "print(\"exp.states[0]:\", experience.states[0])\n",
    "print(\"exp.actions[0]:\", experience.actions[0])\n",
    "print(\"exp.rewards[0]:\", experience.rewards[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q4I7769qyCi4"
   },
   "source": [
    "**Training the agent on the new experience**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GZcKxkkUyCXi",
    "outputId": "eed388c1-ded8-41cd-f5a7-1edfc28ae110"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31416, 13, 4, 4)"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "q_agent_experience_6_2_1 = load_experience(h5py.File('drive/My Drive/q_agent_experience_6_2_1', 'r'))\n",
    "q_agent_experience_6_2_1.states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mu42nPcJzjjS",
    "outputId": "ece55581-87db-4278-c023-499253ec85f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246/246 [==============================] - 1s 5ms/step - loss: 7962.8262\n"
     ]
    }
   ],
   "source": [
    "# Run only a single epoch of training (as it is not known whether the experience data is good)\n",
    "\n",
    "agent_6_2.train(q_agent_experience_6_2_1, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YMEtIZplz8Ba",
    "outputId": "56e57b18-269a-49ce-8c64-e439a6b824ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: drive/My Drive/model_6_2_1/assets\n"
     ]
    }
   ],
   "source": [
    "agent_6_2.model.save('drive/My Drive/model_6_2_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluating the updated agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cYAcCPjU0fDz",
    "outputId": "8960a98a-c6f3-4d82-c88e-ab69e7eb6a5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 100/1000 ...\n",
      "Game 200/1000 ...\n",
      "Game 300/1000 ...\n",
      "Game 400/1000 ...\n",
      "Game 500/1000 ...\n",
      "Game 600/1000 ...\n",
      "Game 700/1000 ...\n",
      "Game 800/1000 ...\n",
      "Game 900/1000 ...\n",
      "Game 1000/1000 ...\n",
      "epsilon = 0.5\n",
      "Number of games:  1000\n",
      "Total number of moves:  7640\n",
      "n_games_reward: -75430\n",
      "avg_reward_per_game: -75\n",
      "wins/games: 0.003\n",
      "games_stopped: 13\n",
      "rewards_list[:100]: [-1, -1, -1, -2, -1, -1, -13, -1, -1, -14, -1, -1, -14, -1, -1, -1, -1, -3, -1124, -1, -1, -1, -1, -1, -1, -1, -1, -12, -2, -1, -2, -2, -1, -1, -1, -1018, -1, -210, -1, -1, -210, -2, -12, -1, -1, -1, -1, -1, -1, -1, -1, -15, -1, -1, -1, -2, -1, -1, -2, -2, -1, -3, -2, -1, -1, -1, -1, -1, -210, -1, -12, -1, -2, -1, -1, -1, -2, -2, -1, -1, -1, -12, -1, -1, -1, -1, -1, -12, -3, -1, -1, -2, -1, -1, -1, -1, -1, -1, -1, -1]\n"
     ]
    }
   ],
   "source": [
    "# Number of games: 1000\n",
    "# epsilon = 0.5\n",
    "# avg_reward_per_game: -75\n",
    "# Total number of moves: 7640\n",
    "# wins/games: 0.003\n",
    "# games_stopped: 13\n",
    "\n",
    "\n",
    "n_games = 1000\n",
    "total_moves = 0\n",
    "n_games_reward = 0\n",
    "rewards_list = []\n",
    "wins = 0\n",
    "games_stopped = 0\n",
    "\n",
    "for n in range(n_games):\n",
    "    if (n + 1) % 100 == 0:\n",
    "        print(\"Game %d/%d ...\" % (n + 1, n_games))\n",
    "    \n",
    "    agent = DeepQAgent.new_agent(agent_6_2.model, 4, 4)\n",
    "    (env, percept) = Environment.new_game(4, 4, 0.2, True)\n",
    "    #env.visualize()\n",
    "    #percept.show()\n",
    "    total_reward = 0\n",
    "    num_moves = 0\n",
    "\n",
    "    agent.set_epsilon(0.5)\n",
    "    \n",
    "    while not percept.is_terminated:\n",
    "        (agent, next_action) = agent.select_action(percept)\n",
    "        #print(\"next_action:\", next_action.show())\n",
    "        #agent.agent_state.show()\n",
    "\n",
    "        (env, percept) = env.apply_action(next_action)\n",
    "        #env.visualize()\n",
    "        #percept.show()\n",
    "        total_reward += percept.reward\n",
    "        num_moves += 1\n",
    "        if num_moves > 199:\n",
    "            games_stopped += 1\n",
    "            break\n",
    "    \n",
    "    if total_reward > 0:\n",
    "        wins += 1\n",
    "    n_games_reward += total_reward\n",
    "    total_moves += num_moves\n",
    "    rewards_list.append(total_reward)\n",
    "\n",
    "print(\"epsilon =\", agent.epsilon)\n",
    "print(\"Number of games: \", n_games)\n",
    "print(\"Total number of moves: \", total_moves)\n",
    "print(\"n_games_reward:\", n_games_reward)\n",
    "print(\"avg_reward_per_game: %.f\" % (n_games_reward / n_games))\n",
    "print(\"wins/games: %.3f\" % (wins / n_games))\n",
    "print(\"games_stopped:\", games_stopped)\n",
    "print(\"rewards_list[:100]:\", rewards_list[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate a new training set - 15,000 games (using DeepQAgent)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "jqn4eq-W42km"
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model('drive/My Drive/model_6_2_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gAgiYtGR7bow",
    "outputId": "d1d45959-ca9e-4203-ff9a-a2c20e35675c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 100/15000 ...\n",
      "Game 200/15000 ...\n",
      "Game 300/15000 ...\n",
      "Game 400/15000 ...\n",
      "Game 500/15000 ...\n",
      "Game 600/15000 ...\n",
      "Game 700/15000 ...\n",
      "Game 800/15000 ...\n",
      "Game 900/15000 ...\n",
      "Game 1000/15000 ...\n",
      "Game 1100/15000 ...\n",
      "Game 1200/15000 ...\n",
      "Game 1300/15000 ...\n",
      "Game 1400/15000 ...\n",
      "Game 1500/15000 ...\n",
      "Game 1600/15000 ...\n",
      "Game 1700/15000 ...\n",
      "Game 1800/15000 ...\n",
      "Game 1900/15000 ...\n",
      "Game 2000/15000 ...\n",
      "Game 2100/15000 ...\n",
      "Game 2200/15000 ...\n",
      "Game 2300/15000 ...\n",
      "Game 2400/15000 ...\n",
      "Game 2500/15000 ...\n",
      "Game 2600/15000 ...\n",
      "Game 2700/15000 ...\n",
      "Game 2800/15000 ...\n",
      "Game 2900/15000 ...\n",
      "Game 3000/15000 ...\n",
      "Game 3100/15000 ...\n",
      "Game 3200/15000 ...\n",
      "Game 3300/15000 ...\n",
      "Game 3400/15000 ...\n",
      "Game 3500/15000 ...\n",
      "Game 3600/15000 ...\n",
      "Game 3700/15000 ...\n",
      "Game 3800/15000 ...\n",
      "Game 3900/15000 ...\n",
      "Game 4000/15000 ...\n",
      "Game 4100/15000 ...\n",
      "Game 4200/15000 ...\n",
      "Game 4300/15000 ...\n",
      "Game 4400/15000 ...\n",
      "Game 4500/15000 ...\n",
      "Game 4600/15000 ...\n",
      "Game 4700/15000 ...\n",
      "Game 4800/15000 ...\n",
      "Game 4900/15000 ...\n",
      "Game 5000/15000 ...\n",
      "Game 5100/15000 ...\n",
      "Game 5200/15000 ...\n",
      "Game 5300/15000 ...\n",
      "Game 5400/15000 ...\n",
      "Game 5500/15000 ...\n",
      "Game 5600/15000 ...\n",
      "Game 5700/15000 ...\n",
      "Game 5800/15000 ...\n",
      "Game 5900/15000 ...\n",
      "Game 6000/15000 ...\n",
      "Game 6100/15000 ...\n",
      "Game 6200/15000 ...\n",
      "Game 6300/15000 ...\n",
      "Game 6400/15000 ...\n",
      "Game 6500/15000 ...\n",
      "Game 6600/15000 ...\n",
      "Game 6700/15000 ...\n",
      "Game 6800/15000 ...\n",
      "Game 6900/15000 ...\n",
      "Game 7000/15000 ...\n",
      "Game 7100/15000 ...\n",
      "Game 7200/15000 ...\n",
      "Game 7300/15000 ...\n",
      "Game 7400/15000 ...\n",
      "Game 7500/15000 ...\n",
      "Game 7600/15000 ...\n",
      "Game 7700/15000 ...\n",
      "Game 7800/15000 ...\n",
      "Game 7900/15000 ...\n",
      "Game 8000/15000 ...\n",
      "Game 8100/15000 ...\n",
      "Game 8200/15000 ...\n",
      "Game 8300/15000 ...\n",
      "Game 8400/15000 ...\n",
      "Game 8500/15000 ...\n",
      "Game 8600/15000 ...\n",
      "Game 8700/15000 ...\n",
      "Game 8800/15000 ...\n",
      "Game 8900/15000 ...\n",
      "Game 9000/15000 ...\n",
      "Game 9100/15000 ...\n",
      "Game 9200/15000 ...\n",
      "Game 9300/15000 ...\n",
      "Game 9400/15000 ...\n",
      "Game 9500/15000 ...\n",
      "Game 9600/15000 ...\n",
      "Game 9700/15000 ...\n",
      "Game 9800/15000 ...\n",
      "Game 9900/15000 ...\n",
      "Game 10000/15000 ...\n",
      "Game 10100/15000 ...\n",
      "Game 10200/15000 ...\n",
      "Game 10300/15000 ...\n",
      "Game 10400/15000 ...\n",
      "Game 10500/15000 ...\n",
      "Game 10600/15000 ...\n",
      "Game 10700/15000 ...\n",
      "Game 10800/15000 ...\n",
      "Game 10900/15000 ...\n",
      "Game 11000/15000 ...\n",
      "Game 11100/15000 ...\n",
      "Game 11200/15000 ...\n",
      "Game 11300/15000 ...\n",
      "Game 11400/15000 ...\n",
      "Game 11500/15000 ...\n",
      "Game 11600/15000 ...\n",
      "Game 11700/15000 ...\n",
      "Game 11800/15000 ...\n",
      "Game 11900/15000 ...\n",
      "Game 12000/15000 ...\n",
      "Game 12100/15000 ...\n",
      "Game 12200/15000 ...\n",
      "Game 12300/15000 ...\n",
      "Game 12400/15000 ...\n",
      "Game 12500/15000 ...\n",
      "Game 12600/15000 ...\n",
      "Game 12700/15000 ...\n",
      "Game 12800/15000 ...\n",
      "Game 12900/15000 ...\n",
      "Game 13000/15000 ...\n",
      "Game 13100/15000 ...\n",
      "Game 13200/15000 ...\n",
      "Game 13300/15000 ...\n",
      "Game 13400/15000 ...\n",
      "Game 13500/15000 ...\n",
      "Game 13600/15000 ...\n",
      "Game 13700/15000 ...\n",
      "Game 13800/15000 ...\n",
      "Game 13900/15000 ...\n",
      "Game 14000/15000 ...\n",
      "Game 14100/15000 ...\n",
      "Game 14200/15000 ...\n",
      "Game 14300/15000 ...\n",
      "Game 14400/15000 ...\n",
      "Game 14500/15000 ...\n",
      "Game 14600/15000 ...\n",
      "Game 14700/15000 ...\n",
      "Game 14800/15000 ...\n",
      "Game 14900/15000 ...\n",
      "Game 15000/15000 ...\n",
      "Number of games: 15000\n",
      "Total number of moves: 103770\n",
      "n_games_reward: -1218880\n",
      "avg_reward_per_game: -81\n",
      "wins/games: 0.003\n",
      "games_stopped: 130\n",
      "exp.states.shape: (103770, 13, 4, 4)\n",
      "exp.actions.shape: (103770,)\n",
      "exp.rewards.shape: (103770,)\n",
      "exp.states[0]: [[[1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]]\n",
      "exp.actions[0]: 2\n",
      "exp.rewards[0]: -1\n"
     ]
    }
   ],
   "source": [
    "# Number of games: 15,000\n",
    "# epsilon = 0.5\n",
    "# avg_reward_per_game: -81\n",
    "# Total number of moves: 103770\n",
    "# wins/games: 0.003\n",
    "# games_stopped: 130\n",
    "\n",
    "\n",
    "import h5py\n",
    "\n",
    "\n",
    "n_games = 15000\n",
    "total_moves = 0\n",
    "n_games_reward = 0\n",
    "wins = 0\n",
    "\n",
    "collector = ExperienceCollector()\n",
    "\n",
    "for i in range(n_games):\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(\"Game %d/%d ...\" % (i + 1, n_games))\n",
    "    \n",
    "    agent = DeepQAgent.new_agent(model, 4, 4)\n",
    "    (env, percept) = Environment.new_game(4, 4, 0.2, True)\n",
    "    total_reward = 0\n",
    "    num_moves = 0\n",
    "    \n",
    "    agent.set_epsilon(0.5)\n",
    "\n",
    "    while not percept.is_terminated:\n",
    "        agent.set_collector(collector)\n",
    "        (agent, next_action) = agent.select_action(percept)\n",
    "        (env, percept) = env.apply_action(next_action)\n",
    "        collector.record_reward(percept.reward) # add reward to collector\n",
    "        total_reward += percept.reward\n",
    "        num_moves += 1\n",
    "        if num_moves > 199:\n",
    "            games_stopped += 1\n",
    "            break\n",
    "        \n",
    "        #print(\"Game %d/%d\" % (i + 1, n_games))\n",
    "        #print(\"Total reward:\", total_reward)\n",
    "        #print(\"Moves per episode:\", num_moves)\n",
    "    \n",
    "    if total_reward > 0:\n",
    "        wins += 1\n",
    "    n_games_reward += total_reward\n",
    "    total_moves += num_moves\n",
    "\n",
    "print(\"Number of games:\", n_games)\n",
    "print(\"Total number of moves:\", total_moves)\n",
    "print(\"n_games_reward:\", n_games_reward)\n",
    "print(\"avg_reward_per_game: %.f\" % (n_games_reward / n_games))\n",
    "print(\"wins/games: %.3f\" % (wins / n_games))\n",
    "print(\"games_stopped:\", games_stopped)\n",
    "\n",
    "experience = collector.to_buffer()\n",
    "with h5py.File('drive/My Drive/q_agent_experience_6_2_2', 'w') as exp_out:\n",
    "    experience.serialize(exp_out)\n",
    "\n",
    "print(\"exp.states.shape:\", experience.states.shape)\n",
    "print(\"exp.actions.shape:\", experience.actions.shape)\n",
    "print(\"exp.rewards.shape:\", experience.rewards.shape)\n",
    "print(\"exp.states[0]:\", experience.states[0])\n",
    "print(\"exp.actions[0]:\", experience.actions[0])\n",
    "print(\"exp.rewards[0]:\", experience.rewards[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Continue training, use new experience data (1 epoch)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "60JP-78eK9bL",
    "outputId": "ab76d571-2c25-4990-e62b-7342357f65ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(103770, 13, 4, 4)"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "q_agent_experience_6_2_2 = load_experience(h5py.File('drive/My Drive/q_agent_experience_6_2_2', 'r'))\n",
    "q_agent_experience_6_2_2.states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fvAJm4wSLDXp",
    "outputId": "c8ba9a6c-4805-44eb-a37d-dcd2231003d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "811/811 [==============================] - 3s 4ms/step - loss: 6895.7012\n"
     ]
    }
   ],
   "source": [
    "agent_6_2.train(q_agent_experience_6_2_2, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lU7pFhBXLyDD",
    "outputId": "f59f1134-900b-4ab9-c9dd-200f4da1956d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: drive/My Drive/model_6_2_2/assets\n"
     ]
    }
   ],
   "source": [
    "agent_6_2.model.save('drive/My Drive/model_6_2_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate (epsilon values: 0.5 and 0.35)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "kw0SxiLtKuNQ"
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model('drive/My Drive/model_6_2_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nU6IIkMyMFdd",
    "outputId": "a867c1ca-71ea-494d-ca9c-ef175346aa3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 100/1000 ...\n",
      "Game 200/1000 ...\n",
      "Game 300/1000 ...\n",
      "Game 400/1000 ...\n",
      "Game 500/1000 ...\n",
      "Game 600/1000 ...\n",
      "Game 700/1000 ...\n",
      "Game 800/1000 ...\n",
      "Game 900/1000 ...\n",
      "Game 1000/1000 ...\n",
      "epsilon = 0.5\n",
      "Number of games:  1000\n",
      "Total number of moves:  7557\n",
      "n_games_reward: -92307\n",
      "avg_reward_per_game: -92\n",
      "wins/games: 0.002\n",
      "games_stopped: 9\n",
      "rewards_list[:100]: [-12, -1, -2, -1, -3, -2, -12, -1, -1, -12, -2, -1, -1, -210, -1, -2, -1, -1, -1, -1, -12, -3, -14, -1001, -1, -13, -1, -1, -12, -2, -1, -1, -15, -1, -1, -2, -1, -1, -1, -1, -1, -2, -1, -30, -1, -2, -1, -1, -1, -1, -12, -13, -1, -1, -3, -1, -12, -2, -15, -2, -2, -1, -13, -1, -2, -1001, -1, -1, -2, -15, -1, -1, -1, -1, -1, -1, -1, -1020, -1, -2, -1, -1, -1, -13, -5, -1, -3, -3, -1, -1, -1, -2, -1, -1, -1, -3, -1, -1, -1, -13]\n"
     ]
    }
   ],
   "source": [
    "# epsilon = 0.5\n",
    "\n",
    "# Number of games: 1000\n",
    "# avg_reward_per_game: -92\n",
    "# Total number of moves:  7557\n",
    "# wins/games: 0.002\n",
    "# games_stopped: 9\n",
    "\n",
    "\n",
    "n_games = 1000\n",
    "total_moves = 0\n",
    "n_games_reward = 0\n",
    "rewards_list = []\n",
    "wins = 0\n",
    "games_stopped = 0\n",
    "\n",
    "for i in range(n_games):\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(\"Game %d/%d ...\" % (i + 1, n_games))\n",
    "    \n",
    "    agent = DeepQAgent.new_agent(model, 4, 4)\n",
    "    (env, percept) = Environment.new_game(4, 4, 0.2, True)\n",
    "    #env.visualize()\n",
    "    #percept.show()\n",
    "    total_reward = 0\n",
    "    num_moves = 0\n",
    "\n",
    "    agent.set_epsilon(0.5)\n",
    "    \n",
    "    while not percept.is_terminated:\n",
    "        (agent, next_action) = agent.select_action(percept)\n",
    "        #print(\"next_action:\", next_action.show())\n",
    "        #agent.agent_state.show()\n",
    "\n",
    "        (env, percept) = env.apply_action(next_action)\n",
    "        #env.visualize()\n",
    "        #percept.show()\n",
    "        total_reward += percept.reward\n",
    "        num_moves += 1\n",
    "        if num_moves > 199:\n",
    "            games_stopped += 1\n",
    "            break\n",
    "    \n",
    "    if total_reward > 0:\n",
    "        wins += 1\n",
    "    n_games_reward += total_reward\n",
    "    total_moves += num_moves\n",
    "    rewards_list.append(total_reward)\n",
    "\n",
    "print(\"epsilon =\", agent.epsilon)\n",
    "print(\"Number of games: \", n_games)\n",
    "print(\"Total number of moves: \", total_moves)\n",
    "print(\"n_games_reward:\", n_games_reward)\n",
    "print(\"avg_reward_per_game: %.f\" % (n_games_reward / n_games))\n",
    "print(\"wins/games: %.3f\" % (wins / n_games))\n",
    "print(\"games_stopped:\", games_stopped)\n",
    "print(\"rewards_list[:100]:\", rewards_list[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V7yhvVyKUzxB",
    "outputId": "1b98df55-fc83-44a1-e619-2e9800d1e2fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "953\n",
      "967\n"
     ]
    }
   ],
   "source": [
    "# Only two wins during 1,000 games:\n",
    "\n",
    "for r in rewards_list:\n",
    "    if r > 0:\n",
    "        print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lmxRxQ3MU8nQ",
    "outputId": "7edc5c40-ddab-40b6-a16b-dd9fef47a6c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 100/1000 ...\n",
      "Game 200/1000 ...\n",
      "Game 300/1000 ...\n",
      "Game 400/1000 ...\n",
      "Game 500/1000 ...\n",
      "Game 600/1000 ...\n",
      "Game 700/1000 ...\n",
      "Game 800/1000 ...\n",
      "Game 900/1000 ...\n",
      "Game 1000/1000 ...\n",
      "epsilon = 0.35\n",
      "Number of games:  1000\n",
      "Total number of moves:  5900\n",
      "n_games_reward: -58150\n",
      "avg_reward_per_game: -58\n",
      "wins/games: 0.000\n",
      "games_stopped: 8\n",
      "rewards_list[:100]: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -3, -1, -1, -1, -1001, -1, -1, -2, -1, -1, -3, -1, -1, -1, -1, -1, -2, -14, -31, -1, -1, -1, -13, -1, -1001, -1, -73, -2, -1, -1, -1, -12, -1, -1, -12, -12, -1, -1, -1, -1, -12, -1, -1, -1, -1, -1, -1, -2, -1, -13, -2, -1, -12, -1, -14, -1024, -2, -1, -1, -1, -1, -1, -1, -12, -1, -2, -1, -1001, -2, -1010, -1, -1, -1, -1, -1, -1139, -1, -1, -1, -1, -1, -3, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n"
     ]
    }
   ],
   "source": [
    "# epsilon = 0.35\n",
    "\n",
    "# Number of games:  1000\n",
    "# avg_reward_per_game: -58\n",
    "# Total number of moves:  5900\n",
    "# wins/games: 0.000\n",
    "# games_stopped: 8\n",
    "\n",
    "# The average score per episode is better than for epsilon of 0.5, but there were no wins at all\n",
    "\n",
    "\n",
    "n_games = 1000\n",
    "total_moves = 0\n",
    "n_games_reward = 0\n",
    "rewards_list = []\n",
    "wins = 0\n",
    "games_stopped = 0\n",
    "\n",
    "for i in range(n_games):\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(\"Game %d/%d ...\" % (i + 1, n_games))\n",
    "    \n",
    "    agent = DeepQAgent.new_agent(model, 4, 4)\n",
    "    (env, percept) = Environment.new_game(4, 4, 0.2, True)\n",
    "    #env.visualize()\n",
    "    #percept.show()\n",
    "    total_reward = 0\n",
    "    num_moves = 0\n",
    "\n",
    "    agent.set_epsilon(0.35)\n",
    "    \n",
    "    while not percept.is_terminated:\n",
    "        (agent, next_action) = agent.select_action(percept)\n",
    "        #print(\"next_action:\", next_action.show())\n",
    "        #agent.agent_state.show()\n",
    "\n",
    "        (env, percept) = env.apply_action(next_action)\n",
    "        #env.visualize()\n",
    "        #percept.show()\n",
    "        total_reward += percept.reward\n",
    "        num_moves += 1\n",
    "        if num_moves > 199:\n",
    "            games_stopped += 1\n",
    "            break\n",
    "    \n",
    "    if total_reward > 0:\n",
    "        wins += 1\n",
    "    n_games_reward += total_reward\n",
    "    total_moves += num_moves\n",
    "    rewards_list.append(total_reward)\n",
    "\n",
    "print(\"epsilon =\", agent.epsilon)\n",
    "print(\"Number of games: \", n_games)\n",
    "print(\"Total number of moves: \", total_moves)\n",
    "print(\"n_games_reward:\", n_games_reward)\n",
    "print(\"avg_reward_per_game: %.f\" % (n_games_reward / n_games))\n",
    "print(\"wins/games: %.3f\" % (wins / n_games))\n",
    "print(\"games_stopped:\", games_stopped)\n",
    "print(\"rewards_list[:100]:\", rewards_list[:100])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "deep_q_agent - Draft 1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
