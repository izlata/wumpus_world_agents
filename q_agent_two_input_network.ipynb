{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KsyB11VKCT0K"
   },
   "source": [
    "## Wumpus World - DeepQAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4n7LOkOjCT0L"
   },
   "source": [
    "## Action-value network with two inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** To run the code, using a GPU is needed (as the model layers have the 'channels_first' data format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q-learning: using an action-value network with two inputs (the states and actions) and one output (the action-value). Epsilon-greedy policy was used.**\n",
    "\n",
    "The encoded state goes through several convolutional layers. The proposed action goes into a separate input. The output of the convolutional layers is combined with the proposed action and passed through a dense layer.\n",
    "\n",
    "Belief state is encoded as a 3-D tensor using 13 feature planes (each plane is a grid_height x grid_width matrix). The state shape is (13, grid_height, grid_width). See the `DeepQAgent.encode_belief_state()` function.\n",
    "\n",
    "Feature planes:\n",
    "- Plane 1 - location of the agent\n",
    "- Plane 2 - visited locations\n",
    "- Plane 3 - stench locations\n",
    "- Plane 4 - breeze locations\n",
    "- Planes 5-8 - orientation of the agent\n",
    "- Plane 9 - does the agent have gold?\n",
    "- Plane 10 - does the agent perceive a glitter?\n",
    "- Plane 11 - does the agent have an arrow?\n",
    "- Plane 12 - have the agent heard a scream?\n",
    "- Plane 13 - does the agent perceive a bump?\n",
    "\n",
    "The experience data generated by the probabilistic agent `ProbAgent` was used as the first experience set to train the `DeepQAgent`. Please see the notebook **\"prob_agent_collect_experience.ipynb\"**\n",
    "\n",
    "The `DeepQAgent` learned to climb out of the cave without gold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The `Orientation` class: orientation of the Agent (north, south, east, west)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "tkhP0hufZ2Ct"
   },
   "outputs": [],
   "source": [
    "import enum\n",
    "\n",
    "\n",
    "class Orientation(enum.Enum):\n",
    "    north = 1\n",
    "    south = 2\n",
    "    east = 3\n",
    "    west = 4\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def turn_left(self):\n",
    "        dict_turn_left = {\n",
    "            Orientation.north: Orientation.west, \n",
    "            Orientation.south: Orientation.east, \n",
    "            Orientation.east: Orientation.north, \n",
    "            Orientation.west: Orientation.south\n",
    "        }\n",
    "        new_orientation = dict_turn_left.get(self)\n",
    "        return new_orientation\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def turn_right(self):\n",
    "        dict_turn_right = {\n",
    "            Orientation.north: Orientation.east, \n",
    "            Orientation.south: Orientation.west, \n",
    "            Orientation.east: Orientation.south, \n",
    "            Orientation.west: Orientation.north\n",
    "        }\n",
    "        new_orientation = dict_turn_right.get(self)\n",
    "        return new_orientation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The `Action` class**\n",
    "\n",
    "The agent can move forward, turn left by 90 degrees, or turn right by 90 degrees. \n",
    "\n",
    "The action Grab can be used to pick up the gold if it is in the same square as the agent. \n",
    "\n",
    "The action Shoot can be used to fire an arrow in a straight line in the direction the agent is facing, the arrow continues until it either kills the Wumpus or hits a wall. The Agent has only one arrow. \n",
    "\n",
    "The action Climb, can be used to climb out of the cave, but only from the start square."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "IKX-bA7sZ2Ct"
   },
   "outputs": [],
   "source": [
    "class Action():\n",
    "    def __init__(self, is_forward=False, is_turn_left=False, is_turn_right=False, \n",
    "                 is_shoot=False, is_grab=False, is_climb=False):\n",
    "        assert is_forward ^ is_turn_left ^ is_turn_right ^ is_shoot ^ is_grab ^ is_climb\n",
    "        self.is_forward = is_forward\n",
    "        self.is_turn_left = is_turn_left\n",
    "        self.is_turn_right = is_turn_right\n",
    "        self.is_shoot = is_shoot\n",
    "        self.is_grab = is_grab\n",
    "        self.is_climb = is_climb\n",
    "    \n",
    "    @classmethod\n",
    "    def forward(cls):\n",
    "        return Action(is_forward=True)\n",
    "    \n",
    "    @classmethod\n",
    "    def turn_left(cls):\n",
    "        return Action(is_turn_left=True)\n",
    "    \n",
    "    @classmethod\n",
    "    def turn_right(cls):\n",
    "        return Action(is_turn_right=True)\n",
    "    \n",
    "    @classmethod\n",
    "    def shoot(cls):\n",
    "        return Action(is_shoot=True)\n",
    "    \n",
    "    @classmethod\n",
    "    def grab(cls):\n",
    "        return Action(is_grab=True)\n",
    "    \n",
    "    @classmethod\n",
    "    def climb(cls):\n",
    "        return Action(is_climb=True)\n",
    "    \n",
    "    def show(self):\n",
    "        if self.is_forward:\n",
    "            action_str = \"forward\"\n",
    "        elif self.is_turn_left:\n",
    "            action_str = \"turn_left\"\n",
    "        elif self.is_turn_right:\n",
    "            action_str = \"turn_right\"\n",
    "        elif self.is_shoot:\n",
    "            action_str = \"shoot\"\n",
    "        elif self.is_grab:\n",
    "            action_str = \"grab\"\n",
    "        else:\n",
    "            action_str = \"climb\"\n",
    "        return action_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The `Coords` class** \n",
    "\n",
    "Each square on the grid has two coordinates: x (column) and y (row). The start square is `Coords(x=0, y=0)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "M4f_M3FEZ2Cu"
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "class Coords(namedtuple('Coords', 'x y')):\n",
    "    def adjacent_cells(self, grid_width, grid_height):\n",
    "        neighbors = []\n",
    "        if self.x > 0: # to left\n",
    "            neighbors.append(Coords(self.x - 1, self.y))\n",
    "        if self.x < (grid_width - 1): # to right\n",
    "            neighbors.append(Coords(self.x + 1, self.y))\n",
    "        if self.y > 0: # below\n",
    "            neighbors.append(Coords(self.x, self.y - 1))\n",
    "        if self.y < (grid_height - 1): # above\n",
    "            neighbors.append(Coords(self.x, self.y + 1))\n",
    "        return neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The `Percept` class**\n",
    "\n",
    "The Agent has five sensors.\n",
    "\n",
    "- In the square containing the Wumpus and in the directly (not diagonally) adjacent squares, the Agent will receive a Stench.\n",
    "- In the squares directly adjacent to a pit, the Agent will perceive a Breeze.\n",
    "- In the square where the gold is, the Agent will perceive a Glitter.\n",
    "- When an Agent walks into a wall it will perceive a Bump.\n",
    "- When the Wumpus is killed, the Agent will hear a Scream.\n",
    "\n",
    "The percept also contains the reward calculated by the environment after each agent's action : +1000 for climbing out of the cave with the gold, -1000 for falling into a pit or being eaten by the Wumpus, -1 for each action taken and -10 for using the arrow.\n",
    "\n",
    "`Percept.is_terminated`: The game ends either when the Agent dies or when the Agent climbs out of the cave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "O7Q1t6vVZ2Cu"
   },
   "outputs": [],
   "source": [
    "class Percept():\n",
    "    def __init__(self, stench, breeze, glitter, bump, scream, is_terminated, reward):\n",
    "        self.stench = stench\n",
    "        self.breeze = breeze\n",
    "        self.glitter = glitter\n",
    "        self.bump = bump\n",
    "        self.scream = scream\n",
    "        self.is_terminated = is_terminated\n",
    "        self.reward = reward\n",
    "    \n",
    "    def show(self):\n",
    "        print(\"stench: {}, breeze: {}, glitter: {}, bump: {}, scream: {}, is_terminated: {}, reward: {}\"\n",
    "              .format(self.stench, self.breeze, self.glitter, self.bump, self.scream, self.is_terminated, self.reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The `AgentState` class**\n",
    "\n",
    "Information about the Agent: location, orientation and whether the Agent is alive, has gold and has arrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "4h9b2yVKZ2Cu"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "class AgentState():\n",
    "    def __init__(self, location=Coords(0, 0), orientation=Orientation.east, has_gold=False, has_arrow=True, is_alive=True):\n",
    "        self.location = location\n",
    "        self.orientation = orientation\n",
    "        self.has_gold = has_gold\n",
    "        self.has_arrow = has_arrow\n",
    "        self.is_alive = is_alive\n",
    "    \n",
    "    def turn_left(self):\n",
    "        new_state = copy.deepcopy(self)\n",
    "        new_state.orientation = new_state.orientation.turn_left\n",
    "        return new_state\n",
    "    \n",
    "    def turn_right(self):\n",
    "        new_state = copy.deepcopy(self)\n",
    "        new_state.orientation = new_state.orientation.turn_right\n",
    "        return new_state\n",
    "    \n",
    "    def forward(self, grid_width, grid_height):\n",
    "        if self.orientation == Orientation.north:\n",
    "            new_loc = Coords(self.location.x, min(grid_height - 1, self.location.y + 1))\n",
    "        elif self.orientation == Orientation.south:\n",
    "            new_loc = Coords(self.location.x, max(0, self.location.y - 1))\n",
    "        elif self.orientation == Orientation.east:\n",
    "            new_loc = Coords(min(grid_width - 1, self.location.x + 1), self.location.y)\n",
    "        else:\n",
    "            new_loc = Coords(max(0, self.location.x - 1), self.location.y) # if Orientation.west\n",
    "        new_state = copy.deepcopy(self)\n",
    "        new_state.location = new_loc\n",
    "        return new_state\n",
    "    \n",
    "    def apply_move_action(self, action, grid_width, grid_height):\n",
    "        if action.is_forward:\n",
    "            return self.forward(grid_width, grid_height)\n",
    "        if action.is_turn_left:\n",
    "            return self.turn_left()\n",
    "        if action.is_turn_right:\n",
    "            return self.turn_right()\n",
    "        if action.is_shoot:\n",
    "            return self.use_arrow()\n",
    "        if action.is_climb:\n",
    "            return self\n",
    "    \n",
    "    def use_arrow(self):\n",
    "        new_state = copy.deepcopy(self)\n",
    "        new_state.has_arrow = False\n",
    "        return new_state\n",
    "    \n",
    "    def show(self):\n",
    "        print(\"location: {}, orientation: {}, has_gold: {}, has_arrow: {}, is_alive: {}\"\n",
    "              .format(self.location, self.orientation, self.has_gold, self.has_arrow, self.is_alive))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functions to create the list of all locations on the board and to generate the locations of gold, wumpus and pits**\n",
    "\n",
    "The locations of the gold and the Wumpus are chosen randomly, with a uniform distribution, from the squares other than the start square. \n",
    "\n",
    "Each square other than the start can be a pit, with probability = `pit_prob`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "0wfqYWG6Z2Cu"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "# Create a list with all locations\n",
    "\n",
    "def list_all_locations(grid_width, grid_height):\n",
    "    all_cells = []\n",
    "    for x in range(grid_width):\n",
    "        for y in range(grid_height):\n",
    "            all_cells.append(Coords(x, y))\n",
    "    return all_cells\n",
    "\n",
    "\n",
    "\n",
    "# Create locations for gold and wumpus\n",
    "\n",
    "def random_location_except_origin(grid_width, grid_height):\n",
    "    locations = list_all_locations(grid_width, grid_height)\n",
    "    locations.remove(Coords(0, 0))\n",
    "    return random.choice(locations)\n",
    "\n",
    "\n",
    "\n",
    "# Create pit locations\n",
    "\n",
    "def create_pit_locations(grid_width, grid_height, pit_prob):\n",
    "    locations = list_all_locations(grid_width, grid_height)\n",
    "    locations.remove(Coords(0, 0))\n",
    "    pit_locations = [loc for loc in locations if random.random() < pit_prob]\n",
    "    return pit_locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The `Environment` class**\n",
    "\n",
    "An environment is initialized with these parameters:\n",
    "- width of the grid\n",
    "- height of the grid\n",
    "- allow climb without gold\n",
    "- pit probability: the probability of a pit being added to each square except (0, 0)\n",
    "\n",
    "The standard game is an initialization of (4, 4, True, 0.2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "CP7K90GpZ2Cu"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "class Environment():\n",
    "    def __init__(self, grid_width, grid_height, pit_prob, allow_climb_without_gold, agent, pit_locations,\n",
    "                 terminated, wumpus_loc, wumpus_alive, gold_loc):\n",
    "        self.grid_width = grid_width\n",
    "        self.grid_height = grid_height\n",
    "        self.pit_prob = pit_prob\n",
    "        self.allow_climb_without_gold = allow_climb_without_gold\n",
    "        self.agent = agent\n",
    "        self.pit_locations = pit_locations\n",
    "        self.terminated = terminated\n",
    "        self.wumpus_loc = wumpus_loc\n",
    "        self.wumpus_alive = wumpus_alive\n",
    "        self.gold_loc = gold_loc\n",
    "    \n",
    "    \n",
    "    def is_pit_at(self, coords):\n",
    "        return coords in self.pit_locations\n",
    "    \n",
    "    \n",
    "    def is_wumpus_at(self, coords):\n",
    "        return coords == self.wumpus_loc\n",
    "    \n",
    "    \n",
    "    def is_agent_at(self, coords):\n",
    "        return coords == self.agent.location\n",
    "    \n",
    "    \n",
    "    def is_glitter(self):\n",
    "        return self.gold_loc == self.agent.location\n",
    "    \n",
    "    \n",
    "    def is_gold_at(self, coords):\n",
    "        return coords == self.gold_loc\n",
    "    \n",
    "    \n",
    "    def wumpus_in_line_of_fire(self):\n",
    "        if self.agent.orientation == Orientation.west:\n",
    "            return self.agent.location.x > self.wumpus_loc.x and self.agent.location.y == self.wumpus_loc.y\n",
    "        if self.agent.orientation == Orientation.east:\n",
    "            return self.agent.location.x < self.wumpus_loc.x and self.agent.location.y == self.wumpus_loc.y\n",
    "        if self.agent.orientation == Orientation.south:\n",
    "            return self.agent.location.x == self.wumpus_loc.x and self.agent.location.y > self.wumpus_loc.y\n",
    "        if self.agent.orientation == Orientation.north:\n",
    "            return self.agent.location.x == self.wumpus_loc.x and self.agent.location.y < self.wumpus_loc.y\n",
    "    \n",
    "    \n",
    "    def kill_attempt_successful(self):\n",
    "        return self.agent.has_arrow and self.wumpus_alive and self.wumpus_in_line_of_fire()\n",
    "    \n",
    "    \n",
    "    def is_pit_adjacent(self, coords):\n",
    "        for cell in coords.adjacent_cells(self.grid_width, self.grid_height):\n",
    "            if cell in self.pit_locations:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    \n",
    "    def is_wumpus_adjacent(self, coords):\n",
    "        for cell in coords.adjacent_cells(self.grid_width, self.grid_height):\n",
    "            if self.is_wumpus_at(cell):\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    \n",
    "    def is_breeze(self):\n",
    "        return self.is_pit_adjacent(self.agent.location)\n",
    "    \n",
    "    \n",
    "    def is_stench(self):\n",
    "        return self.is_wumpus_adjacent(self.agent.location) or self.is_wumpus_at(self.agent.location)\n",
    "    \n",
    "    \n",
    "    def apply_action(self, action):\n",
    "        if self.terminated:\n",
    "            return (self, Percept(False, False, False, False, False, True, 0))\n",
    "        else:\n",
    "            if action.is_forward:\n",
    "                moved_agent = self.agent.forward(self.grid_width, self.grid_height)\n",
    "                death = (self.is_wumpus_at(moved_agent.location) and self.wumpus_alive) or self.is_pit_at(moved_agent.location)\n",
    "                new_agent = copy.deepcopy(moved_agent)\n",
    "                new_agent.is_alive = not death\n",
    "                new_gold_loc = new_agent.location if self.agent.has_gold else self.gold_loc\n",
    "                new_env = Environment(self.grid_width, self.grid_height, self.pit_prob, self.allow_climb_without_gold, \n",
    "                                      new_agent, self.pit_locations, death, self.wumpus_loc, self.wumpus_alive, new_gold_loc)\n",
    "                percept = Percept(new_env.is_stench(), new_env.is_breeze(), new_env.is_glitter(), \n",
    "                                  new_agent.location == self.agent.location, False, death, \n",
    "                                  -1 if new_agent.is_alive else -1001)\n",
    "                return (new_env, percept)\n",
    "            \n",
    "            if action.is_turn_left:\n",
    "                new_env = Environment(self.grid_width, self.grid_height, self.pit_prob, self.allow_climb_without_gold, \n",
    "                                      self.agent.turn_left(), self.pit_locations, self.terminated, self.wumpus_loc, \n",
    "                                      self.wumpus_alive, self.gold_loc)\n",
    "                percept = Percept(self.is_stench(), self.is_breeze(), self.is_glitter(), False, False, False, -1)\n",
    "                return (new_env, percept)\n",
    "            \n",
    "            if action.is_turn_right:\n",
    "                new_env = Environment(self.grid_width, self.grid_height, self.pit_prob, self.allow_climb_without_gold, \n",
    "                                      self.agent.turn_right(), self.pit_locations, self.terminated, self.wumpus_loc, \n",
    "                                      self.wumpus_alive, self.gold_loc)\n",
    "                percept = Percept(self.is_stench(), self.is_breeze(), self.is_glitter(), False, False, False, -1)\n",
    "                return (new_env, percept)\n",
    "            \n",
    "            if action.is_grab:\n",
    "                new_agent = copy.deepcopy(self.agent)\n",
    "                new_agent.has_gold = self.is_glitter()\n",
    "                new_gold_loc = new_agent.location if new_agent.has_gold else self.gold_loc\n",
    "                new_env = Environment(self.grid_width, self.grid_height, self.pit_prob, self.allow_climb_without_gold, \n",
    "                                      new_agent, self.pit_locations, self.terminated, self.wumpus_loc, self.wumpus_alive, \n",
    "                                      new_gold_loc)\n",
    "                percept = Percept(self.is_stench(), self.is_breeze(), self.is_glitter(), False, False, False, -1)\n",
    "                return (new_env, percept)\n",
    "            \n",
    "            if action.is_climb:\n",
    "                in_start_loc = self.agent.location == Coords(0, 0)\n",
    "                success = self.agent.has_gold and in_start_loc\n",
    "                is_terminated = success or (self.allow_climb_without_gold and in_start_loc)\n",
    "                new_env = Environment(self.grid_width, self.grid_height, self.pit_prob, self.allow_climb_without_gold, \n",
    "                                      self.agent, self.pit_locations, is_terminated, self.wumpus_loc, self.wumpus_alive, \n",
    "                                      self.gold_loc)\n",
    "                percept = Percept(self.is_stench(), self.is_breeze(), self.is_glitter(), False, False, is_terminated, \n",
    "                                  999 if success else -1)\n",
    "                return (new_env, percept)\n",
    "            \n",
    "            if action.is_shoot:\n",
    "                had_arrow = self.agent.has_arrow\n",
    "                wumpus_killed = self.kill_attempt_successful()\n",
    "                new_agent = copy.deepcopy(self.agent)\n",
    "                new_agent.has_arrow = False\n",
    "                new_env = Environment(self.grid_width, self.grid_height, self.pit_prob, self.allow_climb_without_gold, \n",
    "                                      new_agent, self.pit_locations, self.terminated, self.wumpus_loc, \n",
    "                                      self.wumpus_alive and (not wumpus_killed), self.gold_loc)\n",
    "                percept = Percept(self.is_stench(), self.is_breeze(), self.is_glitter(), False, wumpus_killed, False, \n",
    "                                  -11 if had_arrow else -1)\n",
    "                return (new_env, percept)\n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    def new_game(cls, grid_width, grid_height, pit_prob, allow_climb_without_gold):\n",
    "        new_pit_locations = create_pit_locations(grid_width, grid_height, pit_prob)\n",
    "        new_wumpus_loc = random_location_except_origin(grid_width, grid_height)\n",
    "        new_gold_loc = random_location_except_origin(grid_width, grid_height)\n",
    "        env = Environment(grid_width, grid_height, pit_prob, allow_climb_without_gold, \n",
    "                          AgentState(), new_pit_locations, False, new_wumpus_loc, True, new_gold_loc)\n",
    "        percept = Percept(env.is_stench(), env.is_breeze(), False, False, False, False, 0.0)\n",
    "        return (env, percept)\n",
    "    \n",
    "    \n",
    "    def visualize(self):\n",
    "        wumpus_symbol = \"W\" if self.wumpus_alive else \"w\"\n",
    "        all_rows = []\n",
    "        for y in range(self.grid_height - 1, -1, -1):\n",
    "            row = []\n",
    "            for x in range (self.grid_width):\n",
    "                agent = \"A\" if self.is_agent_at(Coords(x, y)) else \" \"\n",
    "                pit = \"P\" if self.is_pit_at(Coords(x, y)) else \" \"\n",
    "                gold = \"G\" if self.is_gold_at(Coords(x, y)) else \" \"\n",
    "                wumpus = wumpus_symbol if self.is_wumpus_at(Coords(x, y)) else \" \"\n",
    "                cell = agent + pit + gold + wumpus\n",
    "                row.append(cell)\n",
    "            row_str = \"|\".join(row)\n",
    "            all_rows.append(row_str)\n",
    "        final_str = \"\\n\".join(all_rows)\n",
    "        print(final_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functions to encode and decode actions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "jZ2MKdL4Z2Cu"
   },
   "outputs": [],
   "source": [
    "# Convert action to int\n",
    "\n",
    "def encode_action_to_int(action):\n",
    "    if action.is_forward:\n",
    "        action_int = 0\n",
    "    elif action.is_turn_left:\n",
    "        action_int = 1\n",
    "    elif action.is_turn_right:\n",
    "        action_int = 2\n",
    "    elif action.is_shoot:\n",
    "        action_int = 3\n",
    "    elif action.is_grab:\n",
    "        action_int = 4\n",
    "    else: # climb\n",
    "        action_int = 5\n",
    "    return action_int\n",
    "\n",
    "\n",
    "\n",
    "# Convert action index (int) to action\n",
    "\n",
    "def decode_action_index(index):\n",
    "    actions = [Action.forward(), Action.turn_left(), Action.turn_right(), Action.shoot(), Action.grab(), Action.climb()]\n",
    "    return actions[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The `ExperienceBuffer` and `ExperienceCollector` classes: for handling experience data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "-pnJdxPlZ2Cu"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# The ExperienceBuffer class to store the states, actions and rewards as NumPy arrays\n",
    "\n",
    "class ExperienceBuffer:\n",
    "    def __init__(self, states, actions, rewards):\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.rewards = rewards\n",
    "    \n",
    "    def serialize(self, h5file):\n",
    "        h5file.create_group('experience')\n",
    "        h5file['experience'].create_dataset('states', data=self.states)\n",
    "        h5file['experience'].create_dataset('actions', data=self.actions)\n",
    "        h5file['experience'].create_dataset('rewards', data=self.rewards)\n",
    "\n",
    "\n",
    "\n",
    "# Function to load the experience buffer from HDF5 file\n",
    "\n",
    "def load_experience(h5file):\n",
    "    return ExperienceBuffer(\n",
    "        states=np.array(h5file['experience']['states']),\n",
    "        actions=np.array(h5file['experience']['actions']),\n",
    "        rewards=np.array(h5file['experience']['rewards']))\n",
    "\n",
    "\n",
    "\n",
    "# Function to combine experience buffers\n",
    "\n",
    "def combine_experience(buffers):\n",
    "    combined_states = np.concatenate([b.states for b in buffers])\n",
    "    combined_actions = np.concatenate([b.actions for b in buffers])\n",
    "    combined_rewards = np.concatenate([b.rewards for b in buffers])\n",
    "\n",
    "    return ExperienceBuffer(\n",
    "        combined_states,\n",
    "        combined_actions,\n",
    "        combined_rewards)\n",
    "\n",
    "\n",
    "\n",
    "# The ExperienceCollector class to collect all the states, decisions and rewards (as Python lists)\n",
    "\n",
    "class ExperienceCollector:\n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "    \n",
    "    def record_state(self, state):\n",
    "        self.states.append(state)\n",
    "    \n",
    "    def record_action(self, action):\n",
    "        self.actions.append(action)\n",
    "    \n",
    "    def record_reward(self, reward):\n",
    "        self.rewards.append(reward)\n",
    "    \n",
    "    def to_buffer(self):\n",
    "        return ExperienceBuffer(\n",
    "            states=np.array(self.states), \n",
    "            actions=np.array(self.actions), \n",
    "            rewards=np.array(self.rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "BihayXJJZ2Cu"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def select_action(self, percept):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The `DeepQAgent` class: a Q-learning agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Ist8ZWLrZ2Cu"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "\n",
    "class DeepQAgent(Agent):\n",
    "    def __init__(self, model, grid_width, grid_height, agent_state,\n",
    "                 visited_locations, stench_locations, breeze_locations, \n",
    "                 perceives_glitter, heard_scream, perceives_bump):\n",
    "        self.model = model\n",
    "        self.grid_width = grid_width\n",
    "        self.grid_height = grid_height\n",
    "        self.agent_state = agent_state\n",
    "        self.visited_locations = set(visited_locations)\n",
    "        self.stench_locations = set(stench_locations)\n",
    "        self.breeze_locations = set(breeze_locations)\n",
    "        self.perceives_glitter = perceives_glitter\n",
    "        self.heard_scream = heard_scream\n",
    "        self.perceives_bump = perceives_bump\n",
    "        self.epsilon = 0.0\n",
    "        self.collector = None\n",
    "        \n",
    "    \n",
    "    \n",
    "    # Control the epsilon-greedy policy\n",
    "    def set_epsilon(self, epsilon):\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Attach an ExperienceCollector object to record the experience data\n",
    "    def set_collector(self, collector):\n",
    "        self.collector = collector\n",
    "    \n",
    "    \n",
    "    \n",
    "    def select_action(self, percept):\n",
    "        \n",
    "        # Update agent's variables\n",
    "        visiting_new_location = self.agent_state.location not in self.visited_locations\n",
    "        if visiting_new_location:\n",
    "            self.visited_locations.add(self.agent_state.location)\n",
    "        if percept.breeze:\n",
    "            self.breeze_locations.add(self.agent_state.location)\n",
    "        if percept.stench:\n",
    "            self.stench_locations.add(self.agent_state.location)\n",
    "        new_heard_scream = self.heard_scream or percept.scream\n",
    "        self.heard_scream = new_heard_scream\n",
    "        self.perceives_glitter = percept.glitter\n",
    "        self.perceives_bump = percept.bump\n",
    "        \n",
    "        num_actions = 6\n",
    "        \n",
    "        state_tensor = self.encode_belief_state() # encode belief state\n",
    "        state_tensors_list = [state_tensor for i in range(num_actions)] # list with 6 state tensors (the same items)\n",
    "        state_tensors_array = np.array(state_tensors_list)\n",
    "        \n",
    "        # One-hot encode all 6 actions\n",
    "        action_vectors = np.zeros((num_actions, num_actions))\n",
    "        for i in range(num_actions):\n",
    "            action_vectors[i][i] = 1\n",
    "        \n",
    "        # Predict action-values (using two inputs)\n",
    "        values = self.model.predict([state_tensors_array, action_vectors])\n",
    "        values = values.reshape(num_actions) # convert a matrix to a vector\n",
    "        ranked_moves = self.rank_moves_eps_greedy(values) # rank the actions according to the epsilon-greedy policy\n",
    "        action_index = ranked_moves[0] # index of the largest value\n",
    "        if self.collector is not None: # record the state and decision if collecting experience\n",
    "            self.collector.record_state(state=state_tensor)\n",
    "            self.collector.record_action(action_index)\n",
    "        next_action = decode_action_index(action_index) # decode the action from index\n",
    "\n",
    "        if next_action.is_grab:\n",
    "            if percept.glitter and not self.agent_state.has_gold:\n",
    "                self.agent_state.has_gold = True\n",
    "        else:\n",
    "            self.agent_state = self.agent_state.apply_move_action(next_action, self.grid_width, self.grid_height)\n",
    "        return (self, next_action)\n",
    "    \n",
    "    \n",
    "\n",
    "    def rank_moves_eps_greedy(self, values):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            values = np.random.random(values.shape)\n",
    "        ranked_moves = np.argsort(values) # rank the actions from worst to best\n",
    "        # Return actions in best-to-worst order (a reversed vector)\n",
    "        return ranked_moves[::-1]\n",
    "    \n",
    "    \n",
    "    \n",
    "    def train(self, experience, lr=0.01, batch_size=128, epochs=1):\n",
    "        opt = keras.optimizers.Adam(lr=lr)\n",
    "        self.model.compile(loss='mse', optimizer=opt)\n",
    "\n",
    "        n = experience.states.shape[0] # number of experience samples\n",
    "        num_actions = 6\n",
    "        y = np.zeros((n,)) # the target vector with rewards\n",
    "        actions = np.zeros((n, num_actions))\n",
    "        for i in range(n):\n",
    "            action = experience.actions[i]\n",
    "            reward = experience.rewards[i] / 1001.0 # rescale the reward values, so they are in the range from -1 to +1\n",
    "            actions[i][action] = 1 # one_hot encode actions\n",
    "            y[i] = reward\n",
    "\n",
    "        self.model.fit([experience.states, actions], y, batch_size=batch_size, epochs=epochs)\n",
    "    \n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    def new_agent(cls, model, grid_width, grid_height):\n",
    "        return DeepQAgent(model, grid_width, grid_height, AgentState(), set(), set(), set(), False, False, False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Encode belief state using 13 feature planes (each plane is a grid_height x grid_width matrix)\n",
    "    # The state shape is (13, grid_height, grid_width)\n",
    "    \n",
    "    def encode_belief_state(self):\n",
    "        state_tensor = np.zeros((13, self.grid_height, self.grid_width)) # create a 3-D tensor\n",
    "        all_cells = list_all_locations(self.grid_width, self.grid_height)\n",
    "        \n",
    "        # The first plane has a 1 for agent's location and 0s for other locations\n",
    "        state_tensor[0][self.agent_state.location.y][self.agent_state.location.x] = 1\n",
    "        \n",
    "        for cell in all_cells:\n",
    "            if cell in self.visited_locations:\n",
    "                state_tensor[1][cell.y][cell.x] = 1 # 1s for visited locations\n",
    "            if cell in self.stench_locations:\n",
    "                state_tensor[2][cell.y][cell.x] = 1 # 1s for stench locations\n",
    "            if cell in self.breeze_locations:\n",
    "                state_tensor[3][cell.y][cell.x] = 1 # 1s for breeze locations\n",
    "        \n",
    "        if self.agent_state.orientation == Orientation.north: # a plane filled with 1s if Orientation.north\n",
    "            state_tensor[4] = 1\n",
    "        elif self.agent_state.orientation == Orientation.south: # a plane filled with 1s if Orientation.south\n",
    "            state_tensor[5] = 1\n",
    "        elif self.agent_state.orientation == Orientation.east: # a plane filled with 1s if Orientation.east\n",
    "            state_tensor[6] = 1\n",
    "        else: # a plane filled with 1s if Orientation.west\n",
    "            state_tensor[7] = 1\n",
    "        \n",
    "        if self.agent_state.has_gold: # a plane filled with 1s if agent has gold, and 0s otherwise\n",
    "            state_tensor[8] = 1\n",
    "        if self.perceives_glitter: # a plane filled with 1s if agent perceives glitter, and 0s otherwise\n",
    "            state_tensor[9] = 1\n",
    "        if self.agent_state.has_arrow: # a plane filled with 1s if agent has arrow, and 0s otherwise\n",
    "            state_tensor[10] = 1\n",
    "        if self.heard_scream: # a plane filled with 1s if wumpus is not alive, and 0s otherwise\n",
    "            state_tensor[11] = 1\n",
    "        if self.perceives_bump: # a plane filled with 1s if agent perceives bump, and 0s otherwise\n",
    "            state_tensor[12] = 1\n",
    "        \n",
    "        return state_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function to create an action-value network with two inputs (states and actions) and one output (the action-value)**\n",
    "\n",
    "The encoded state goes through several convolutional layers. The proposed action goes into a separate input. The output of the convolutional layers is combined with the proposed action and passed through a dense layer.\n",
    "\n",
    "The output layer is Dense(1, activation='tanh'). The rewards used for training are divided by 1001.0, so that they are in the range from -1 to +1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "T_BHG79bZ2Cv"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, concatenate\n",
    "from tensorflow.keras.layers import ZeroPadding2D, Conv2D, BatchNormalization, Activation\n",
    "\n",
    "\n",
    "def create_action_value_network(state_shape):\n",
    "    state_input = Input(shape=state_shape, name='state_input')\n",
    "    action_input = Input(shape=(6,), name='action_input')\n",
    "    \n",
    "    conv1a = ZeroPadding2D((1, 1), data_format='channels_first')(state_input)\n",
    "    conv1b = Conv2D(64, (3, 3), data_format='channels_first')(conv1a)\n",
    "    conv1c = BatchNormalization(axis=1)(conv1b)\n",
    "    conv1d = Activation('relu')(conv1c)\n",
    "    \n",
    "    conv2a = ZeroPadding2D((1, 1), data_format='channels_first')(conv1d)\n",
    "    conv2b = Conv2D(64, (3, 3), data_format='channels_first')(conv2a)\n",
    "    conv2c = BatchNormalization(axis=1)(conv2b)\n",
    "    conv2d = Activation('relu')(conv2c)\n",
    "    \n",
    "    conv3a = ZeroPadding2D((1, 1), data_format='channels_first')(conv2d)\n",
    "    conv3b = Conv2D(48, (3, 3), data_format='channels_first')(conv3a)\n",
    "    conv3c = BatchNormalization(axis=1)(conv3b)\n",
    "    conv3d = Activation('relu')(conv3c)\n",
    "\n",
    "    conv4a = ZeroPadding2D((1, 1), data_format='channels_first')(conv3d)\n",
    "    conv4b = Conv2D(32, (3, 3), data_format='channels_first')(conv4a)\n",
    "    conv4c = BatchNormalization(axis=1)(conv4b)\n",
    "    conv4d = Activation('relu')(conv4c)\n",
    "    \n",
    "    flat = Flatten()(conv4d)\n",
    "    processed_state = Dense(512, activation='relu')(flat)\n",
    "    \n",
    "    state_and_action = concatenate([action_input, processed_state])\n",
    "    hidden_layer = Dense(256, activation='relu')(state_and_action)\n",
    "    value_output = Dense(1, activation='tanh')(hidden_layer)\n",
    "    \n",
    "    model = Model(inputs=[state_input, action_input], outputs=value_output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting experience, training and evaluating the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W02cch8alFDk",
    "outputId": "311def05-b258-49bb-e25a-9233e202fcae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rUnVHVpWCT0P"
   },
   "source": [
    "### Train the DeepQAgent from the experience data generated by the probabilistic agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wCj1DMy3fAka",
    "outputId": "6a829bb4-5870-4a60-a450-64c9401cf7c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76528, 13, 4, 4)\n",
      "(76398, 13, 4, 4)\n"
     ]
    }
   ],
   "source": [
    "# Load the experience datasets generated by the probabilistic agent (5,000 games each)\n",
    "\n",
    "import h5py\n",
    "\n",
    "prob_agent_experience_02 = load_experience(h5py.File('drive/My Drive/prob_agent_experience_02', 'r'))\n",
    "print(prob_agent_experience_02.states.shape)\n",
    "\n",
    "prob_agent_experience_03 = load_experience(h5py.File('drive/My Drive/prob_agent_experience_03', 'r'))\n",
    "print(prob_agent_experience_03.states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HiCc0_etDbXH",
    "outputId": "bfbdb598-2a02-4c42-d77b-c72e8d36278b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(152926, 13, 4, 4)\n"
     ]
    }
   ],
   "source": [
    "# Combine two experience buffers\n",
    "\n",
    "prob_agent_experience = combine_experience([prob_agent_experience_02, prob_agent_experience_03])\n",
    "print(prob_agent_experience.states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ufMl384OpESC",
    "outputId": "3d773af2-9c94-4b7e-fb10-0155c322e680"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "state_input (InputLayer)        [(None, 13, 4, 4)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2D)  (None, 13, 6, 6)     0           state_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 64, 4, 4)     7552        zero_padding2d[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 64, 4, 4)     256         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 64, 4, 4)     0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D (None, 64, 6, 6)     0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 64, 4, 4)     36928       zero_padding2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 64, 4, 4)     256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 64, 4, 4)     0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPadding2D (None, 64, 6, 6)     0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 48, 4, 4)     27696       zero_padding2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 48, 4, 4)     192         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 48, 4, 4)     0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPadding2D (None, 48, 6, 6)     0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 4, 4)     13856       zero_padding2d_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 4, 4)     128         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 4, 4)     0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 512)          0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "action_input (InputLayer)       [(None, 6)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          262656      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 518)          0           action_input[0][0]               \n",
      "                                                                 dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          132864      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            257         dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 482,641\n",
      "Trainable params: 482,225\n",
      "Non-trainable params: 416\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create an action-value network\n",
    "\n",
    "model_10_2 = create_action_value_network(prob_agent_experience.states[0].shape)\n",
    "model_10_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ArrBhUlRps6_",
    "outputId": "b355e8b4-d7b4-4bc8-95ef-f3b888f8fcbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1195/1195 [==============================] - 4s 4ms/step - loss: 0.0134\n",
      "Epoch 2/100\n",
      "1195/1195 [==============================] - 4s 4ms/step - loss: 0.0062\n",
      "Epoch 3/100\n",
      "1195/1195 [==============================] - 4s 4ms/step - loss: 0.0091\n",
      "Epoch 4/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0059\n",
      "Epoch 5/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0059\n",
      "Epoch 6/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0058\n",
      "Epoch 7/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0058\n",
      "Epoch 8/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0058\n",
      "Epoch 9/100\n",
      "1195/1195 [==============================] - 4s 4ms/step - loss: 0.0058\n",
      "Epoch 10/100\n",
      "1195/1195 [==============================] - 4s 4ms/step - loss: 0.0057\n",
      "Epoch 11/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0057\n",
      "Epoch 12/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0057\n",
      "Epoch 13/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0057\n",
      "Epoch 14/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0057\n",
      "Epoch 15/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0056\n",
      "Epoch 16/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0056\n",
      "Epoch 17/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0056\n",
      "Epoch 18/100\n",
      "1195/1195 [==============================] - 4s 4ms/step - loss: 0.0056\n",
      "Epoch 19/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0056\n",
      "Epoch 20/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0055\n",
      "Epoch 21/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0055\n",
      "Epoch 22/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0056\n",
      "Epoch 23/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0054\n",
      "Epoch 24/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0054\n",
      "Epoch 25/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0054\n",
      "Epoch 26/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0054\n",
      "Epoch 27/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0054\n",
      "Epoch 28/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0054\n",
      "Epoch 29/100\n",
      "1195/1195 [==============================] - 4s 4ms/step - loss: 0.0054\n",
      "Epoch 30/100\n",
      "1195/1195 [==============================] - 5s 4ms/step - loss: 0.0053\n",
      "Epoch 31/100\n",
      "1195/1195 [==============================] - 4s 4ms/step - loss: 0.0053\n",
      "Epoch 32/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0053\n",
      "Epoch 33/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0052\n",
      "Epoch 34/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0053\n",
      "Epoch 35/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0052\n",
      "Epoch 36/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0051\n",
      "Epoch 37/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0052\n",
      "Epoch 38/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0051\n",
      "Epoch 39/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0051\n",
      "Epoch 40/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0051\n",
      "Epoch 41/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0051\n",
      "Epoch 42/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0051\n",
      "Epoch 43/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0050\n",
      "Epoch 44/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0050\n",
      "Epoch 45/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0050\n",
      "Epoch 46/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0050\n",
      "Epoch 47/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0050\n",
      "Epoch 48/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0049\n",
      "Epoch 49/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0049\n",
      "Epoch 50/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0049\n",
      "Epoch 51/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0049\n",
      "Epoch 52/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0049\n",
      "Epoch 53/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0050\n",
      "Epoch 54/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0049\n",
      "Epoch 55/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0048\n",
      "Epoch 56/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0048\n",
      "Epoch 57/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0048\n",
      "Epoch 58/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0048\n",
      "Epoch 59/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0048\n",
      "Epoch 60/100\n",
      "1195/1195 [==============================] - 4s 4ms/step - loss: 0.0048\n",
      "Epoch 61/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0049\n",
      "Epoch 62/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0048\n",
      "Epoch 63/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0048\n",
      "Epoch 64/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0047\n",
      "Epoch 65/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0048\n",
      "Epoch 66/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0048\n",
      "Epoch 67/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0048\n",
      "Epoch 68/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0048\n",
      "Epoch 69/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0047\n",
      "Epoch 70/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0047\n",
      "Epoch 71/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0047\n",
      "Epoch 72/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0047\n",
      "Epoch 73/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0048\n",
      "Epoch 74/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0047\n",
      "Epoch 75/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0047\n",
      "Epoch 76/100\n",
      "1195/1195 [==============================] - 4s 4ms/step - loss: 0.0047\n",
      "Epoch 77/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0047\n",
      "Epoch 78/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0047\n",
      "Epoch 79/100\n",
      "1195/1195 [==============================] - 4s 4ms/step - loss: 0.0048\n",
      "Epoch 80/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0047\n",
      "Epoch 81/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0047\n",
      "Epoch 82/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0047\n",
      "Epoch 83/100\n",
      "1195/1195 [==============================] - 4s 4ms/step - loss: 0.0047\n",
      "Epoch 84/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0047\n",
      "Epoch 85/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0047\n",
      "Epoch 86/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0047\n",
      "Epoch 87/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0046\n",
      "Epoch 88/100\n",
      "1195/1195 [==============================] - 4s 4ms/step - loss: 0.0047\n",
      "Epoch 89/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0047\n",
      "Epoch 90/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0046\n",
      "Epoch 91/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0047\n",
      "Epoch 92/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0047\n",
      "Epoch 93/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0046\n",
      "Epoch 94/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0047\n",
      "Epoch 95/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0046\n",
      "Epoch 96/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0047\n",
      "Epoch 97/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0046\n",
      "Epoch 98/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0046\n",
      "Epoch 99/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0046\n",
      "Epoch 100/100\n",
      "1195/1195 [==============================] - 4s 3ms/step - loss: 0.0047\n"
     ]
    }
   ],
   "source": [
    "# Create a DeepQAgent (4x4 grid) and train it on the experience data\n",
    "# Adam optimizer, lr=0.001, epochs=100\n",
    "\n",
    "agent_10_2 = DeepQAgent.new_agent(model_10_2, 4, 4)\n",
    "agent_10_2.train(prob_agent_experience, lr=0.001, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7oW_usqSqkGb",
    "outputId": "57f9d958-cbed-44b8-e88a-272c4b91d6b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: drive/My Drive/model_10_2/assets\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model as the \"model_10_2\"\n",
    "\n",
    "agent_10_2.model.save('drive/My Drive/model_10_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gn3DAT_2CT0Q"
   },
   "source": [
    "### Evaluating the updated agent: running 1,000 games and calculating the average score per game\n",
    "\n",
    "**Only if the agent climbs out with gold, it is counted as a win.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "APtT2Cj_rH7z"
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model('drive/My Drive/model_10_2')\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b3QkJXVaxx_5",
    "outputId": "f16476ae-41fd-4330-dc4e-96bf00133e76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_action: climb\n",
      "next_action: turn_right\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "next_action: turn_right\n",
      "next_action: climb\n",
      "next_action: turn_right\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "next_action: turn_right\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "next_action: turn_right\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "next_action: turn_right\n",
      "next_action: climb\n",
      "next_action: turn_right\n",
      "next_action: climb\n",
      "next_action: turn_right\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "next_action: turn_right\n",
      "next_action: climb\n",
      "next_action: turn_right\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "next_action: turn_right\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "next_action: turn_right\n",
      "next_action: climb\n",
      "next_action: turn_right\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "next_action: turn_right\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "next_action: turn_right\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "next_action: turn_right\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "next_action: turn_right\n",
      "next_action: climb\n",
      "next_action: turn_right\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "next_action: turn_right\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "next_action: turn_right\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "next_action: turn_right\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "next_action: turn_right\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "next_action: turn_right\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "next_action: turn_right\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "next_action: turn_right\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "next_action: turn_right\n",
      "next_action: climb\n",
      "next_action: turn_right\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "next_action: turn_right\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "next_action: turn_right\n",
      "next_action: climb\n",
      "next_action: turn_right\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "next_action: turn_right\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "next_action: turn_right\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "next_action: turn_right\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "next_action: turn_right\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "next_action: turn_right\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "next_action: turn_right\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "next_action: turn_right\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "Game 100/100 ...\n",
      "next_action: turn_right\n",
      "next_action: climb\n",
      "epsilon = 0.0\n",
      "Number of games:  100\n",
      "Total number of moves:  138\n",
      "n_games_reward: -138\n",
      "avg_reward_per_game: -1.38\n",
      "wins/games: 0.000\n",
      "games_stopped: 0\n",
      "rewards_list[:100]: [-1, -2, -1, -2, -2, -1, -2, -1, -2, -1, -2, -2, -2, -1, -1, -2, -2, -1, -2, -1, -1, -2, -2, -1, -1, -2, -1, -1, -1, -1, -1, -1, -2, -1, -1, -1, -2, -1, -1, -2, -2, -1, -2, -1, -1, -1, -1, -2, -1, -1, -1, -2, -1, -1, -2, -1, -2, -1, -2, -1, -2, -1, -2, -2, -1, -2, -1, -2, -2, -1, -2, -1, -1, -2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -2, -1, -1, -1, -2, -1, -2, -1, -1, -2, -1, -2, -1, -1, -2]\n"
     ]
    }
   ],
   "source": [
    "# epsilon = 0.0\n",
    "# Run 100 games to see the actions\n",
    "# The agent climbs out without gold immediately after the game start, or turns right and then climbs out\n",
    "\n",
    "\n",
    "n_games = 100\n",
    "total_moves = 0\n",
    "n_games_reward = 0\n",
    "rewards_list = []\n",
    "wins = 0\n",
    "games_stopped = 0\n",
    "\n",
    "for i in range(n_games):\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(\"Game %d/%d ...\" % (i + 1, n_games))\n",
    "    \n",
    "    agent = DeepQAgent.new_agent(model, 4, 4)\n",
    "    (env, percept) = Environment.new_game(4, 4, 0.2, True)\n",
    "    #env.visualize()\n",
    "    #percept.show()\n",
    "    total_reward = 0\n",
    "    num_moves = 0\n",
    "\n",
    "    #agent.set_epsilon(0.5)\n",
    "    \n",
    "    while not percept.is_terminated:\n",
    "        (agent, next_action) = agent.select_action(percept)\n",
    "        print(\"next_action:\", next_action.show())\n",
    "        #agent.agent_state.show()\n",
    "\n",
    "        (env, percept) = env.apply_action(next_action)\n",
    "        #env.visualize()\n",
    "        #percept.show()\n",
    "        total_reward += percept.reward\n",
    "        num_moves += 1\n",
    "        if num_moves > 199:\n",
    "            games_stopped += 1\n",
    "            break\n",
    "    \n",
    "    if total_reward > 0: # only if the agent climbs out with gold, it is counted as a win\n",
    "        wins += 1\n",
    "    n_games_reward += total_reward\n",
    "    total_moves += num_moves\n",
    "    rewards_list.append(total_reward)\n",
    "\n",
    "print(\"epsilon =\", agent.epsilon)\n",
    "print(\"Number of games: \", n_games)\n",
    "print(\"Total number of moves: \", total_moves)\n",
    "print(\"n_games_reward:\", n_games_reward)\n",
    "print(\"avg_reward_per_game: %.2f\" % (n_games_reward / n_games))\n",
    "print(\"wins/games: %.3f\" % (wins / n_games))\n",
    "print(\"games_stopped:\", games_stopped)\n",
    "print(\"rewards_list[:100]:\", rewards_list[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate the updated agent (1,000 games), epsilon=0.0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6xbs4pNgDJZt",
    "outputId": "32b26390-f6d4-4b77-e1f6-37b02d453878"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 100/1000 ...\n",
      "Game 200/1000 ...\n",
      "Game 300/1000 ...\n",
      "Game 400/1000 ...\n",
      "Game 500/1000 ...\n",
      "Game 600/1000 ...\n",
      "Game 700/1000 ...\n",
      "Game 800/1000 ...\n",
      "Game 900/1000 ...\n",
      "Game 1000/1000 ...\n",
      "epsilon = 0.0\n",
      "Number of games:  1000\n",
      "Total number of moves:  1360\n",
      "n_games_reward: -1360\n",
      "avg_reward_per_game: -1.36\n",
      "wins/games: 0.000\n",
      "games_stopped: 0\n",
      "rewards_list[:100]: [-1, -2, -1, -1, -2, -1, -2, -1, -2, -1, -1, -2, -1, -2, -2, -1, -1, -1, -1, -1, -1, -2, -2, -2, -2, -1, -1, -1, -1, -1, -1, -2, -2, -1, -1, -1, -1, -1, -1, -1, -1, -2, -2, -2, -1, -1, -2, -2, -1, -1, -1, -1, -1, -2, -2, -1, -2, -1, -1, -1, -2, -2, -2, -1, -2, -1, -1, -1, -1, -2, -1, -1, -1, -1, -1, -1, -2, -1, -1, -2, -2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -2, -1, -1, -1, -1, -1, -2, -1]\n"
     ]
    }
   ],
   "source": [
    "# epsilon = 0.0\n",
    "\n",
    "# Number of games:  1000\n",
    "# avg_reward_per_game: -1.36\n",
    "# Total number of moves:  1036\n",
    "# wins/games: 0.000\n",
    "# games_stopped: 0 (after 200 moves)\n",
    "\n",
    "\n",
    "n_games = 1000\n",
    "total_moves = 0\n",
    "n_games_reward = 0\n",
    "rewards_list = []\n",
    "wins = 0\n",
    "games_stopped = 0 \n",
    "\n",
    "for i in range(n_games):\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(\"Game %d/%d ...\" % (i + 1, n_games))\n",
    "    \n",
    "    agent = DeepQAgent.new_agent(model, 4, 4)\n",
    "    (env, percept) = Environment.new_game(4, 4, 0.2, True)\n",
    "    #env.visualize()\n",
    "    #percept.show()\n",
    "    total_reward = 0\n",
    "    num_moves = 0\n",
    "\n",
    "    #agent.set_epsilon(0.5)\n",
    "    \n",
    "    while not percept.is_terminated:\n",
    "        (agent, next_action) = agent.select_action(percept)\n",
    "        #print(\"next_action:\", next_action.show())\n",
    "        #agent.agent_state.show()\n",
    "\n",
    "        (env, percept) = env.apply_action(next_action)\n",
    "        #env.visualize()\n",
    "        #percept.show()\n",
    "        total_reward += percept.reward\n",
    "        num_moves += 1\n",
    "        if num_moves > 199:\n",
    "            games_stopped += 1\n",
    "            break\n",
    "    \n",
    "    if total_reward > 0: # only if the agent climbs out with gold, it is counted as a win\n",
    "        wins += 1\n",
    "    n_games_reward += total_reward\n",
    "    total_moves += num_moves\n",
    "    rewards_list.append(total_reward)\n",
    "\n",
    "print(\"epsilon =\", agent.epsilon)\n",
    "print(\"Number of games: \", n_games)\n",
    "print(\"Total number of moves: \", total_moves)\n",
    "print(\"n_games_reward:\", n_games_reward)\n",
    "print(\"avg_reward_per_game: %.2f\" % (n_games_reward / n_games))\n",
    "print(\"wins/games: %.3f\" % (wins / n_games))\n",
    "print(\"games_stopped:\", games_stopped)\n",
    "print(\"rewards_list[:100]:\", rewards_list[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate the updated agent (1,000 games), epsilon=0.5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wOIO_c0CswJN",
    "outputId": "4be2a3c5-16f7-46d7-bf07-742cce7c9337"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 100/1000 ...\n",
      "Game 200/1000 ...\n",
      "Game 300/1000 ...\n",
      "Game 400/1000 ...\n",
      "Game 500/1000 ...\n",
      "Game 600/1000 ...\n",
      "Game 700/1000 ...\n",
      "Game 800/1000 ...\n",
      "Game 900/1000 ...\n",
      "Game 1000/1000 ...\n",
      "epsilon = 0.5\n",
      "Number of games: 1000\n",
      "Total number of moves: 8197\n",
      "n_games_reward: -84477\n",
      "avg_reward_per_game: -84\n",
      "wins/games: 0.003\n",
      "games_stopped: 11\n",
      "rewards_list[:100]: [-1, -1, -2, -1, -46, -1, -3, -1, -1, -1, -1, -2, -2, -13, -3, -2, -2, -2, -1076, -2, -2, -2, -2, -1036, -2, -2, -14, -12, -13, -2, -1001, -59, -2, -1, -1, -1, -3, -3, -1, -1, -1005, -2, -2, -1, -2, -1001, -1, -2, -1, -1093, -2, -2, -1, -2, -2, -2, -2, -1, -1, -1015, -2, -2, -2, -3, -82, -2, -3, -1, -1, -1, -4, -1, -12, -1, -1, -3, -14, -1, -1100, -1, -1, -5, -1027, -4, -2, -2, -2, -1, -8, -1, -1, -13, -1, -1, -1, -13, -1, -3, -2, -3]\n"
     ]
    }
   ],
   "source": [
    "# epsilon = 0.5\n",
    "\n",
    "# Number of games:  1000\n",
    "# Average total reward per game = -84\n",
    "# wins/games: 0.003\n",
    "# games_stopped: 11 (after 200 moves)\n",
    "\n",
    "\n",
    "n_games = 1000\n",
    "total_moves = 0\n",
    "n_games_reward = 0\n",
    "rewards_list = []\n",
    "wins = 0\n",
    "games_stopped = 0\n",
    "\n",
    "for i in range(n_games):\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(\"Game %d/%d ...\" % (i + 1, n_games))\n",
    "    \n",
    "    agent = DeepQAgent.new_agent(model, 4, 4)\n",
    "    (env, percept) = Environment.new_game(4, 4, 0.2, True)\n",
    "    #env.visualize()\n",
    "    #percept.show()\n",
    "    total_reward = 0\n",
    "    num_moves = 0\n",
    "\n",
    "    agent.set_epsilon(0.5)\n",
    "    \n",
    "    while not percept.is_terminated:\n",
    "        (agent, next_action) = agent.select_action(percept)\n",
    "        #print(\"next_action:\", next_action.show())\n",
    "        #agent.agent_state.show()\n",
    "\n",
    "        (env, percept) = env.apply_action(next_action)\n",
    "        #env.visualize()\n",
    "        #percept.show()\n",
    "        total_reward += percept.reward\n",
    "        num_moves += 1\n",
    "        if num_moves > 199:\n",
    "            games_stopped += 1\n",
    "            break\n",
    "    \n",
    "    if total_reward > 0:\n",
    "        wins += 1\n",
    "    n_games_reward += total_reward\n",
    "    total_moves += num_moves\n",
    "    rewards_list.append(total_reward)\n",
    "\n",
    "print(\"epsilon =\", agent.epsilon)\n",
    "print(\"Number of games:\", n_games)\n",
    "print(\"Total number of moves:\", total_moves)\n",
    "print(\"n_games_reward:\", n_games_reward)\n",
    "print(\"avg_reward_per_game: %.f\" % (n_games_reward / n_games))\n",
    "print(\"wins/games: %.3f\" % (wins / n_games))\n",
    "print(\"games_stopped:\", games_stopped)\n",
    "print(\"rewards_list[:100]:\", rewards_list[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LrfZtidKFTXi"
   },
   "source": [
    "### Collect new experience data using the updated DeepQAgent and save it as a file (10,000 games)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iJoMuPABFTLY",
    "outputId": "9872b679-d4dd-4fa3-c18d-9dd872659727"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 100/10000 ...\n",
      "Game 200/10000 ...\n",
      "Game 300/10000 ...\n",
      "Game 400/10000 ...\n",
      "Game 500/10000 ...\n",
      "Game 600/10000 ...\n",
      "Game 700/10000 ...\n",
      "Game 800/10000 ...\n",
      "Game 900/10000 ...\n",
      "Game 1000/10000 ...\n",
      "Game 1100/10000 ...\n",
      "Game 1200/10000 ...\n",
      "Game 1300/10000 ...\n",
      "Game 1400/10000 ...\n",
      "Game 1500/10000 ...\n",
      "Game 1600/10000 ...\n",
      "Game 1700/10000 ...\n",
      "Game 1800/10000 ...\n",
      "Game 1900/10000 ...\n",
      "Game 2000/10000 ...\n",
      "Game 2100/10000 ...\n",
      "Game 2200/10000 ...\n",
      "Game 2300/10000 ...\n",
      "Game 2400/10000 ...\n",
      "Game 2500/10000 ...\n",
      "Game 2600/10000 ...\n",
      "Game 2700/10000 ...\n",
      "Game 2800/10000 ...\n",
      "Game 2900/10000 ...\n",
      "Game 3000/10000 ...\n",
      "Game 3100/10000 ...\n",
      "Game 3200/10000 ...\n",
      "Game 3300/10000 ...\n",
      "Game 3400/10000 ...\n",
      "Game 3500/10000 ...\n",
      "Game 3600/10000 ...\n",
      "Game 3700/10000 ...\n",
      "Game 3800/10000 ...\n",
      "Game 3900/10000 ...\n",
      "Game 4000/10000 ...\n",
      "Game 4100/10000 ...\n",
      "Game 4200/10000 ...\n",
      "Game 4300/10000 ...\n",
      "Game 4400/10000 ...\n",
      "Game 4500/10000 ...\n",
      "Game 4600/10000 ...\n",
      "Game 4700/10000 ...\n",
      "Game 4800/10000 ...\n",
      "Game 4900/10000 ...\n",
      "Game 5000/10000 ...\n",
      "Game 5100/10000 ...\n",
      "Game 5200/10000 ...\n",
      "Game 5300/10000 ...\n",
      "Game 5400/10000 ...\n",
      "Game 5500/10000 ...\n",
      "Game 5600/10000 ...\n",
      "Game 5700/10000 ...\n",
      "Game 5800/10000 ...\n",
      "Game 5900/10000 ...\n",
      "Game 6000/10000 ...\n",
      "Game 6100/10000 ...\n",
      "Game 6200/10000 ...\n",
      "Game 6300/10000 ...\n",
      "Game 6400/10000 ...\n",
      "Game 6500/10000 ...\n",
      "Game 6600/10000 ...\n",
      "Game 6700/10000 ...\n",
      "Game 6800/10000 ...\n",
      "Game 6900/10000 ...\n",
      "Game 7000/10000 ...\n",
      "Game 7100/10000 ...\n",
      "Game 7200/10000 ...\n",
      "Game 7300/10000 ...\n",
      "Game 7400/10000 ...\n",
      "Game 7500/10000 ...\n",
      "Game 7600/10000 ...\n",
      "Game 7700/10000 ...\n",
      "Game 7800/10000 ...\n",
      "Game 7900/10000 ...\n",
      "Game 8000/10000 ...\n",
      "Game 8100/10000 ...\n",
      "Game 8200/10000 ...\n",
      "Game 8300/10000 ...\n",
      "Game 8400/10000 ...\n",
      "Game 8500/10000 ...\n",
      "Game 8600/10000 ...\n",
      "Game 8700/10000 ...\n",
      "Game 8800/10000 ...\n",
      "Game 8900/10000 ...\n",
      "Game 9000/10000 ...\n",
      "Game 9100/10000 ...\n",
      "Game 9200/10000 ...\n",
      "Game 9300/10000 ...\n",
      "Game 9400/10000 ...\n",
      "Game 9500/10000 ...\n",
      "Game 9600/10000 ...\n",
      "Game 9700/10000 ...\n",
      "Game 9800/10000 ...\n",
      "Game 9900/10000 ...\n",
      "Game 10000/10000 ...\n",
      "epsilon = 0.5\n",
      "Number of games: 10000\n",
      "Total number of moves: 73792\n",
      "n_games_reward: -828332\n",
      "avg_reward_per_game: -83\n",
      "wins/games: 0.002\n",
      "games_stopped: 85\n",
      "exp.states.shape: (73792, 13, 4, 4)\n",
      "exp.actions.shape: (73792,)\n",
      "exp.rewards.shape: (73792,)\n",
      "exp.states[0]: [[[1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]]\n",
      "exp.actions[0]: 4\n",
      "exp.rewards[0]: -1\n"
     ]
    }
   ],
   "source": [
    "# 10,000 games\n",
    "# epsilon = 0.5\n",
    "\n",
    "# Average total reward per game = -83\n",
    "# Total number of moves: 73792\n",
    "# wins/games: 0.002\n",
    "# games_stopped: 85 (after 200 moves)\n",
    "\n",
    "\n",
    "import h5py\n",
    "\n",
    "\n",
    "n_games = 10000\n",
    "total_moves = 0\n",
    "n_games_reward = 0\n",
    "wins = 0\n",
    "\n",
    "collector = ExperienceCollector()\n",
    "\n",
    "for i in range(n_games):\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(\"Game %d/%d ...\" % (i + 1, n_games))\n",
    "    \n",
    "    agent = DeepQAgent.new_agent(model, 4, 4)\n",
    "    (env, percept) = Environment.new_game(4, 4, 0.2, True)\n",
    "    total_reward = 0\n",
    "    num_moves = 0\n",
    "    \n",
    "    agent.set_epsilon(0.5)\n",
    "\n",
    "    while not percept.is_terminated:\n",
    "        agent.set_collector(collector)\n",
    "        (agent, next_action) = agent.select_action(percept)\n",
    "        (env, percept) = env.apply_action(next_action)\n",
    "        collector.record_reward(percept.reward) # add reward to collector\n",
    "        total_reward += percept.reward\n",
    "        num_moves += 1\n",
    "        if num_moves > 199:\n",
    "            games_stopped += 1\n",
    "            break\n",
    "        \n",
    "        #print(\"Game %d/%d\" % (i + 1, n_games))\n",
    "        #print(\"Total reward:\", total_reward)\n",
    "        #print(\"Moves per episode:\", num_moves)\n",
    "    \n",
    "    if total_reward > 0:\n",
    "        wins += 1\n",
    "    n_games_reward += total_reward\n",
    "    total_moves += num_moves\n",
    "\n",
    "print(\"epsilon =\", agent.epsilon)\n",
    "print(\"Number of games:\", n_games)\n",
    "print(\"Total number of moves:\", total_moves)\n",
    "print(\"n_games_reward:\", n_games_reward)\n",
    "print(\"avg_reward_per_game: %.f\" % (n_games_reward / n_games))\n",
    "print(\"wins/games: %.3f\" % (wins / n_games))\n",
    "print(\"games_stopped:\", games_stopped)\n",
    "\n",
    "experience = collector.to_buffer()\n",
    "with h5py.File('drive/My Drive/q_agent_experience_10_2_1', 'w') as exp_out:\n",
    "    experience.serialize(exp_out)\n",
    "\n",
    "print(\"exp.states.shape:\", experience.states.shape)\n",
    "print(\"exp.actions.shape:\", experience.actions.shape)\n",
    "print(\"exp.rewards.shape:\", experience.rewards.shape)\n",
    "print(\"exp.states[0]:\", experience.states[0])\n",
    "print(\"exp.actions[0]:\", experience.actions[0])\n",
    "print(\"exp.rewards[0]:\", experience.rewards[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q4I7769qyCi4"
   },
   "source": [
    "### Training the agent on the new experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GZcKxkkUyCXi",
    "outputId": "6e99e1f6-bfdd-4331-d145-e97717ee7b61"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73792, 13, 4, 4)"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Q agent's experience\n",
    "\n",
    "import h5py\n",
    "\n",
    "q_agent_experience_10_2_1 = load_experience(h5py.File('drive/My Drive/q_agent_experience_10_2_1', 'r'))\n",
    "q_agent_experience_10_2_1.states.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train with learning rate = 0.001**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mu42nPcJzjjS",
    "outputId": "d60a7b65-2d5b-40c9-b3aa-6f1dac7aea2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "577/577 [==============================] - 2s 4ms/step - loss: 0.0061\n"
     ]
    }
   ],
   "source": [
    "# Run only a single epoch of training (as it is not known whether the experience data is good)\n",
    "# lr=0.001\n",
    "\n",
    "agent_10_2.train(q_agent_experience_10_2_1, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YMEtIZplz8Ba",
    "outputId": "e3699e8f-b4a3-424a-8299-239c0bcd090b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: drive/My Drive/model_10_2_1/assets\n"
     ]
    }
   ],
   "source": [
    "# Save the model as \"model_10_2_1\"\n",
    "\n",
    "agent_10_2.model.save('drive/My Drive/model_10_2_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l3YPCaGiCT0R"
   },
   "source": [
    "**Evaluating the updated agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "yXBDIKU8WKpB"
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model('drive/My Drive/model_10_2_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WoLuDFLGSrU8",
    "outputId": "22e88b95-06fc-47a3-dbbd-6e9eafa8ec03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "next_action: grab\n",
      "epsilon = 0.0\n",
      "Number of games:  5\n",
      "Total number of moves:  100\n",
      "n_games_reward: -100\n",
      "avg_reward_per_game: -20.00\n",
      "wins/games: 0.000\n",
      "games_stopped: 5\n",
      "rewards_list[:100]: [-20, -20, -20, -20, -20]\n"
     ]
    }
   ],
   "source": [
    "# epsilon = 0.0\n",
    "\n",
    "# Run 5 games to see the actions\n",
    "# The agent chooses the Grab action when in the start location and it is stuck in the start location\n",
    "# It is worse than before (climbing out right away)\n",
    "\n",
    "# max number of moves is set to 20\n",
    "\n",
    "\n",
    "n_games = 5\n",
    "total_moves = 0\n",
    "n_games_reward = 0\n",
    "rewards_list = []\n",
    "wins = 0\n",
    "games_stopped = 0\n",
    "\n",
    "for i in range(n_games):\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(\"Game %d/%d ...\" % (i + 1, n_games))\n",
    "    \n",
    "    agent = DeepQAgent.new_agent(model, 4, 4)\n",
    "    (env, percept) = Environment.new_game(4, 4, 0.2, True)\n",
    "    #env.visualize()\n",
    "    #percept.show()\n",
    "    total_reward = 0\n",
    "    num_moves = 0\n",
    "\n",
    "    #agent.set_epsilon(0.5)\n",
    "    \n",
    "    while not percept.is_terminated:\n",
    "        (agent, next_action) = agent.select_action(percept)\n",
    "        print(\"next_action:\", next_action.show())\n",
    "        #agent.agent_state.show()\n",
    "\n",
    "        (env, percept) = env.apply_action(next_action)\n",
    "        #env.visualize()\n",
    "        #percept.show()\n",
    "        total_reward += percept.reward\n",
    "        num_moves += 1\n",
    "        if num_moves > 19:\n",
    "            games_stopped += 1\n",
    "            break\n",
    "    \n",
    "    if total_reward > 0:\n",
    "        wins += 1\n",
    "    n_games_reward += total_reward\n",
    "    total_moves += num_moves\n",
    "    rewards_list.append(total_reward)\n",
    "\n",
    "print(\"epsilon =\", agent.epsilon)\n",
    "print(\"Number of games: \", n_games)\n",
    "print(\"Total number of moves: \", total_moves)\n",
    "print(\"n_games_reward:\", n_games_reward)\n",
    "print(\"avg_reward_per_game: %.2f\" % (n_games_reward / n_games))\n",
    "print(\"wins/games: %.3f\" % (wins / n_games))\n",
    "print(\"games_stopped:\", games_stopped)\n",
    "print(\"rewards_list[:100]:\", rewards_list[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ywu4hMHVaFC"
   },
   "source": [
    "### Load the \"model_10_2\" and train on the new experience with learning rate = 0.0001 instead of 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vw5BdBwoUTtY",
    "outputId": "fbeff456-1f13-460c-e2a2-9e569678d095"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "577/577 [==============================] - 2s 3ms/step - loss: 0.0075\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model('drive/My Drive/model_10_2')\n",
    "agent = DeepQAgent.new_agent(model, 4,4)\n",
    "agent.train(q_agent_experience_10_2_1, lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sGj75kxAVSCq",
    "outputId": "3e589448-a8a1-4390-b9f8-73a898061887"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_action: climb\n",
      "next_action: turn_right\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "next_action: climb\n",
      "next_action: turn_right\n",
      "next_action: climb\n",
      "epsilon = 0.0\n",
      "Number of games:  5\n",
      "Total number of moves:  7\n",
      "n_games_reward: -7\n",
      "avg_reward_per_game: -1.40\n",
      "wins/games: 0.000\n",
      "games_stopped: 0\n",
      "rewards_list[:100]: [-1, -2, -1, -1, -2]\n"
     ]
    }
   ],
   "source": [
    "# epsilon = 0.0\n",
    "\n",
    "# Run 5 games to see the actions\n",
    "# The agent chooses the climb and the (turn_right, then climb) actions when in the start location \n",
    "# The result with the learning rate of 0.0001 is better than for lr=0.001\n",
    "\n",
    "# max number of moves is set to 20\n",
    "\n",
    "\n",
    "n_games = 5\n",
    "total_moves = 0\n",
    "n_games_reward = 0\n",
    "rewards_list = []\n",
    "wins = 0\n",
    "games_stopped = 0\n",
    "\n",
    "for i in range(n_games):\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(\"Game %d/%d ...\" % (i + 1, n_games))\n",
    "    \n",
    "    agent = DeepQAgent.new_agent(model, 4, 4)\n",
    "    (env, percept) = Environment.new_game(4, 4, 0.2, True)\n",
    "    #env.visualize()\n",
    "    #percept.show()\n",
    "    total_reward = 0\n",
    "    num_moves = 0\n",
    "\n",
    "    #agent.set_epsilon(0.5)\n",
    "    \n",
    "    while not percept.is_terminated:\n",
    "        (agent, next_action) = agent.select_action(percept)\n",
    "        print(\"next_action:\", next_action.show())\n",
    "        #agent.agent_state.show()\n",
    "\n",
    "        (env, percept) = env.apply_action(next_action)\n",
    "        #env.visualize()\n",
    "        #percept.show()\n",
    "        total_reward += percept.reward\n",
    "        num_moves += 1\n",
    "        if num_moves > 19:\n",
    "            games_stopped += 1\n",
    "            break\n",
    "    \n",
    "    if total_reward > 0:\n",
    "        wins += 1\n",
    "    n_games_reward += total_reward\n",
    "    total_moves += num_moves\n",
    "    rewards_list.append(total_reward)\n",
    "\n",
    "print(\"epsilon =\", agent.epsilon)\n",
    "print(\"Number of games: \", n_games)\n",
    "print(\"Total number of moves: \", total_moves)\n",
    "print(\"n_games_reward:\", n_games_reward)\n",
    "print(\"avg_reward_per_game: %.2f\" % (n_games_reward / n_games))\n",
    "print(\"wins/games: %.3f\" % (wins / n_games))\n",
    "print(\"games_stopped:\", games_stopped)\n",
    "print(\"rewards_list[:100]:\", rewards_list[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save this model and evaluate it at epsilon = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AkmUbRSyV-Pw",
    "outputId": "8b7fa43c-b2b3-4d56-f9e3-65e0c3e37f54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: drive/My Drive/model_10_2_2/assets\n"
     ]
    }
   ],
   "source": [
    "# Save the model as the \"model_10_2_2\"\n",
    "\n",
    "model.save('drive/My Drive/model_10_2_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cYAcCPjU0fDz",
    "outputId": "33b654a0-bb52-463e-abee-956c36f6b401"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 100/1000 ...\n",
      "Game 200/1000 ...\n",
      "Game 300/1000 ...\n",
      "Game 400/1000 ...\n",
      "Game 500/1000 ...\n",
      "Game 600/1000 ...\n",
      "Game 700/1000 ...\n",
      "Game 800/1000 ...\n",
      "Game 900/1000 ...\n",
      "Game 1000/1000 ...\n",
      "epsilon = 0.5\n",
      "Number of games:  1000\n",
      "Total number of moves:  7820\n",
      "n_games_reward: -85840\n",
      "avg_reward_per_game: -86\n",
      "wins/games: 0.001\n",
      "games_stopped: 8\n",
      "rewards_list[:100]: [-1, -2, -1, -2, -3, -1, -2, -1, -1, -29, -2, -2, -2, -2, -13, -12, -1, -1002, -3, -2, -12, -14, -2, -4, -2, -1, -1, -1, -7, -1, -1, -1, -15, -2, -1, -1, -27, -2, -1, -12, -1, -1, -199, -1, -2, -2, -1, -1, -1, -2, -1, -2, -2, -13, -1, -1158, -14, -2, -74, -3, -1, -12, -1, -1, -1006, -15, -2, -2, -148, -3, -1, -9, -1, -2, -1, -12, -12, -1, -2, -1053, -1, -1, -3, -210, -13, -50, -1035, -1, -3, -12, -14, -2, -2, -2, -4, -4, -2, -2, -2, -4]\n"
     ]
    }
   ],
   "source": [
    "# epsilon = 0.5\n",
    "\n",
    "# Number of games: 1000\n",
    "# avg_reward_per_game: -86\n",
    "# Total number of moves: 7820\n",
    "# wins/games: 0.001\n",
    "# games_stopped: 8 (after 200 moves)\n",
    "\n",
    "\n",
    "n_games = 1000\n",
    "total_moves = 0\n",
    "n_games_reward = 0\n",
    "rewards_list = []\n",
    "wins = 0\n",
    "games_stopped = 0\n",
    "\n",
    "for n in range(n_games):\n",
    "    if (n + 1) % 100 == 0:\n",
    "        print(\"Game %d/%d ...\" % (n + 1, n_games))\n",
    "    \n",
    "    agent = DeepQAgent.new_agent(model, 4, 4)\n",
    "    (env, percept) = Environment.new_game(4, 4, 0.2, True)\n",
    "    #env.visualize()\n",
    "    #percept.show()\n",
    "    total_reward = 0\n",
    "    num_moves = 0\n",
    "\n",
    "    agent.set_epsilon(0.5)\n",
    "    \n",
    "    while not percept.is_terminated:\n",
    "        (agent, next_action) = agent.select_action(percept)\n",
    "        #print(\"next_action:\", next_action.show())\n",
    "        #agent.agent_state.show()\n",
    "\n",
    "        (env, percept) = env.apply_action(next_action)\n",
    "        #env.visualize()\n",
    "        #percept.show()\n",
    "        total_reward += percept.reward\n",
    "        num_moves += 1\n",
    "        if num_moves > 199:\n",
    "            games_stopped += 1\n",
    "            break\n",
    "    \n",
    "    if total_reward > 0:\n",
    "        wins += 1\n",
    "    n_games_reward += total_reward\n",
    "    total_moves += num_moves\n",
    "    rewards_list.append(total_reward)\n",
    "\n",
    "print(\"epsilon =\", agent.epsilon)\n",
    "print(\"Number of games: \", n_games)\n",
    "print(\"Total number of moves: \", total_moves)\n",
    "print(\"n_games_reward:\", n_games_reward)\n",
    "print(\"avg_reward_per_game: %.f\" % (n_games_reward / n_games))\n",
    "print(\"wins/games: %.3f\" % (wins / n_games))\n",
    "print(\"games_stopped:\", games_stopped)\n",
    "print(\"rewards_list[:100]:\", rewards_list[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the agent at epsilon = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WR5bsh4sYaEs",
    "outputId": "2d86e60e-8fc3-40f3-bf31-4390c2bc4aff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 100/1000 ...\n",
      "Game 200/1000 ...\n",
      "Game 300/1000 ...\n",
      "Game 400/1000 ...\n",
      "Game 500/1000 ...\n",
      "Game 600/1000 ...\n",
      "Game 700/1000 ...\n",
      "Game 800/1000 ...\n",
      "Game 900/1000 ...\n",
      "Game 1000/1000 ...\n",
      "epsilon = 0.0\n",
      "Number of games:  1000\n",
      "Total number of moves:  1427\n",
      "n_games_reward: -1427\n",
      "avg_reward_per_game: -1.43\n",
      "wins/games: 0.000\n",
      "games_stopped: 0\n",
      "rewards_list[:100]: [-1, -1, -2, -1, -1, -1, -1, -1, -1, -2, -2, -2, -1, -1, -1, -2, -1, -2, -2, -1, -2, -1, -1, -2, -1, -2, -2, -1, -1, -1, -2, -1, -1, -2, -1, -1, -1, -1, -2, -2, -1, -1, -2, -2, -1, -1, -2, -1, -1, -2, -1, -1, -1, -1, -1, -2, -1, -1, -1, -1, -1, -2, -2, -1, -1, -2, -2, -1, -2, -2, -2, -2, -1, -2, -2, -1, -1, -2, -1, -2, -1, -1, -1, -1, -1, -1, -2, -1, -2, -2, -2, -1, -1, -2, -2, -1, -1, -2, -1, -1]\n"
     ]
    }
   ],
   "source": [
    "# epsilon = 0.0\n",
    "\n",
    "# Number of games: 1000\n",
    "# avg_reward_per_game: -1.43\n",
    "# Total number of moves: 1427\n",
    "# wins/games: 0.000\n",
    "# games_stopped: 0 (after 200 moves)\n",
    "\n",
    "\n",
    "n_games = 1000\n",
    "total_moves = 0\n",
    "n_games_reward = 0\n",
    "rewards_list = []\n",
    "wins = 0\n",
    "games_stopped = 0\n",
    "\n",
    "for n in range(n_games):\n",
    "    if (n + 1) % 100 == 0:\n",
    "        print(\"Game %d/%d ...\" % (n + 1, n_games))\n",
    "    \n",
    "    agent = DeepQAgent.new_agent(model, 4, 4)\n",
    "    (env, percept) = Environment.new_game(4, 4, 0.2, True)\n",
    "    #env.visualize()\n",
    "    #percept.show()\n",
    "    total_reward = 0\n",
    "    num_moves = 0\n",
    "\n",
    "    #agent.set_epsilon(0.5)\n",
    "    \n",
    "    while not percept.is_terminated:\n",
    "        (agent, next_action) = agent.select_action(percept)\n",
    "        #print(\"next_action:\", next_action.show())\n",
    "        #agent.agent_state.show()\n",
    "\n",
    "        (env, percept) = env.apply_action(next_action)\n",
    "        #env.visualize()\n",
    "        #percept.show()\n",
    "        total_reward += percept.reward\n",
    "        num_moves += 1\n",
    "        if num_moves > 199:\n",
    "            games_stopped += 1\n",
    "            break\n",
    "    \n",
    "    if total_reward > 0:\n",
    "        wins += 1\n",
    "    n_games_reward += total_reward\n",
    "    total_moves += num_moves\n",
    "    rewards_list.append(total_reward)\n",
    "\n",
    "print(\"epsilon =\", agent.epsilon)\n",
    "print(\"Number of games: \", n_games)\n",
    "print(\"Total number of moves: \", total_moves)\n",
    "print(\"n_games_reward:\", n_games_reward)\n",
    "print(\"avg_reward_per_game: %.2f\" % (n_games_reward / n_games))\n",
    "print(\"wins/games: %.3f\" % (wins / n_games))\n",
    "print(\"games_stopped:\", games_stopped)\n",
    "print(\"rewards_list[:100]:\", rewards_list[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S3w6eiawVbTq"
   },
   "source": [
    "### Conclusions\n",
    "\n",
    "\n",
    "- **The DeepQAgent trained on the probabilistic experience data learned to climb out without gold**\n",
    "\n",
    "\n",
    "- **The average score per game was about -85 (if epsilon=0.5) and -1.4 (if epsilon=0.0). The wins percentage was very low (0.3%, 0.1%). To compare, the probabilistic agent got the average score of 266 and 40% of wins (was getting the highest reward quite often)**\n",
    "\n",
    "\n",
    "- **The agent needs further training for improvement**\n",
    "\n",
    "\n",
    "- **The network and DeepQAgent can be used for larger grids**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Copy of q_agent_two_input_network.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
